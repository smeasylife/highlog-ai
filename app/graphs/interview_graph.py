"""ì‹¤ì‹œê°„ ë©´ì ‘ LangGraph êµ¬í˜„

ê¼¬ë¦¬ ì§ˆë¬¸(Tail Questions) ì‹œìŠ¤í…œì„ í†µí•´ ì‹¬ì¸µì ì¸ ë©´ì ‘ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
ìƒíƒœ ì €ì¥ì€ LangGraphì˜ AsyncPostgresSaver Checkpointerê°€ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
"""
from typing import TypedDict, List, Dict, Any, Optional
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.postgres import PostgresSaver
from pydantic import BaseModel, Field
from google import genai
from google.genai import types
from config import settings
from app.database import SessionLocal
from app.models import InterviewSession
from sqlalchemy.sql import func
import logging
import json

logger = logging.getLogger(__name__)


# ==================== State ì •ì˜ ====================

class InterviewState(TypedDict):
    """ë©´ì ‘ ìƒíƒœ"""

    # ê¸°ë³¸ ì„¤ì •
    difficulty: str                    # ë©´ì ‘ ë‚œì´ë„ (Easy, Normal, Hard)
    remaining_time: int                # ë‚¨ì€ ì‹œê°„ (ì´ˆ ë‹¨ìœ„)
    interview_stage: str               # [INTRO, MAIN, WRAP_UP]

    # ëŒ€í™” ì»¨í…ìŠ¤íŠ¸
    current_context: List[int]         # í˜„ì¬ ì§ˆë¬¸/ì£¼ì œì™€ ê´€ë ¨ëœ í•™ìƒë¶€ ì²­í¬ ID ë¦¬ìŠ¤íŠ¸
    current_sub_topic: str             # í˜„ì¬ ì§„í–‰ ì¤‘ì¸ ì„¸ë¶€ ì£¼ì œ
    asked_sub_topics: List[str]        # ì´ë¯¸ ì™„ë£Œëœ ì„¸ë¶€ ì£¼ì œ ë¦¬ìŠ¤íŠ¸

    # ë‚´ë¶€ ìƒíƒœ
    next_action: str                   # [follow_up, new_topic, wrap_up]
    follow_up_count: int               # í˜„ì¬ ì£¼ì œì— ëŒ€í•œ ê¼¬ë¦¬ ì§ˆë¬¸ íšŸìˆ˜

    # ì„¸ì…˜ ì •ë³´
    session_id: int                    # InterviewSession ID (ë°ì´í„°ë² ì´ìŠ¤ ì™¸ë˜í‚¤)
    record_id: int                     # ìƒê¸°ë¶€ ID

    # í˜„ì¬ ì²˜ë¦¬ ì¤‘ì¸ ë‹µë³€ (checkpointì— ì €ì¥í•˜ì§€ ì•ŠìŒ - pass through)
    current_user_answer: str
    current_response_time: int

    # ë§ˆì§€ë§‰ ì§ˆë¬¸/ë‹µë³€ (ë©”ëª¨ë¦¬ìš©, checkpointì— ì €ì¥ë˜ì§€ ì•ŠìŒ)
    last_question: str = ""
    last_answer: str = ""


# ==================== Pydantic ëª¨ë¸ ====================

class AnalyzerDecision(BaseModel):
    """ë¶„ì„ê¸° ê²°ì • ëª¨ë¸ - ê¼¬ë¦¬ì§ˆë¬¸ ì—¬ë¶€ë§Œ íŒë‹¨"""
    action: str = Field(description="ë‹¤ìŒ ì•¡ì…˜ (follow_up, new_topic, wrap_up)")


class GeneratedQuestion(BaseModel):
    """ìƒì„±ëœ ì§ˆë¬¸ ëª¨ë¸"""
    question: str = Field(description="ì§ˆë¬¸ ë‚´ìš©")


# ==================== í•˜ìœ„ ì£¼ì œ ì •ì˜ ====================

SUB_TOPICS = [
    "ì¶œê²°", "ì„±ì ", "ë™ì•„ë¦¬", "ë¦¬ë”ì‹­", 
    "ì¸ì„±/íƒœë„", "ì§„ë¡œ/ììœ¨", "ë…ì„œ", "ë´‰ì‚¬"
]


# ==================== Interview Graph ====================

class InterviewGraph:
    """ì‹¤ì‹œê°„ ë©´ì ‘ LangGraph"""

    def __init__(self):
        # Google GenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.client = genai.Client(api_key=settings.google_api_key)
        self.model = "gemini-2.5-flash"  # Free Tier ë¬´ì œí•œ (LiteëŠ” í•˜ë£¨ 20íšŒ ì œí•œ)
        self.types = types

        # database_url ì €ì¥ (PostgresSaverìš©)
        # postgresql+psycopg2:// â†’ postgresql://
        # postgresql+psycopg:// â†’ postgresql://
        self._conn_string = settings.database_url
        self._conn_string = self._conn_string.replace("postgresql+psycopg2://", "postgresql://", 1)
        self._conn_string = self._conn_string.replace("postgresql+psycopg://", "postgresql://", 1)

        self._graph = None

    def get_graph(self):
        """ê·¸ë˜í”„ ë°˜í™˜ (checkpointer ì—†ì´ ì»´íŒŒì¼)"""
        if self._graph is None:
            # ê·¸ë˜í”„ ë¹Œë“œ ë° ì»´íŒŒì¼ (checkpointer ì—†ì´)
            self._graph = self._build_workflow().compile()

        return self._graph

    def _build_workflow(self) -> StateGraph:
        """Workflow ë¹Œë“œ (ì»´íŒŒì¼ ì „)"""
        workflow = StateGraph(InterviewState)

        # ë…¸ë“œ ì¶”ê°€
        workflow.add_node("analyzer", self.analyzer)
        workflow.add_node("retrieve_new_topic", self.retrieve_new_topic)
        workflow.add_node("follow_up_generator", self.follow_up_generator)
        workflow.add_node("new_question_generator", self.new_question_generator)
        workflow.add_node("wrap_up", self.wrap_up)

        # ì—”íŠ¸ë¦¬ í¬ì¸íŠ¸
        workflow.set_entry_point("analyzer")

        # ì¡°ê±´ë¶€ ì—£ì§€
        workflow.add_conditional_edges(
            "analyzer",
            self.decide_next_action,
            {
                "follow_up": "follow_up_generator",
                "new_topic": "retrieve_new_topic",
                "wrap_up": "wrap_up"
            }
        )

        # ì¼ë°˜ ì—£ì§€
        workflow.add_edge("retrieve_new_topic", "new_question_generator")
        workflow.add_edge("follow_up_generator", END)
        workflow.add_edge("new_question_generator", END)
        workflow.add_edge("wrap_up", END)

        return workflow

    def analyzer(self, state: InterviewState) -> InterviewState:
        """ë‹µë³€ ë¶„ì„ ë° ë‹¤ìŒ ì•¡ì…˜ ê²°ì • (ê¼¬ë¦¬ì§ˆë¬¸ ì—¬ë¶€ë§Œ íŒë‹¨)"""
        db = None
        try:
            logger.info(f"Analyzing answer for topic: {state.get('current_sub_topic', 'INTRO')}")

            # ê¼¬ë¦¬ ì§ˆë¬¸ 1íšŒ ì´ë¯¸ í–ˆìœ¼ë©´ ë¬´ì¡°ê±´ ë‹¤ìŒ ì£¼ì œë¡œ
            if state.get('follow_up_count', 0) >= 1:
                logger.info("Already did follow-up, moving to new topic")
                state['next_action'] = "new_topic"

                # ë‹µë³€ ë¡œê·¸ ì €ì¥ (InterviewSessionì—ë§Œ)
                log_entry = {
                    "question": state.get('last_question', ''),
                    "answer": state.get('current_user_answer', ''),
                    "response_time": state.get('current_response_time', 0),
                    "sub_topic": state.get('current_sub_topic', '')
                }
                self._save_interview_log(state, log_entry)

                # ë§ˆì§€ë§‰ ë‹µë³€ ì—…ë°ì´íŠ¸
                state['last_answer'] = state.get('current_user_answer', '')

                return state

            # í˜„ì¬ ë‹µë³€ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            user_answer = state.get('current_user_answer', '')
            response_time = state.get('current_response_time', 0)

            # ë§ˆì§€ë§‰ ì§ˆë¬¸ ê°€ì ¸ì˜¤ê¸° (stateì˜ last_question ì‚¬ìš©)
            last_question = state.get('last_question', '')

            # ID ë¦¬ìŠ¤íŠ¸ë¡œ í…ìŠ¤íŠ¸ ì¡°íšŒ
            context_chunks = self._get_chunks_by_ids(state.get('current_context', []))
            context_text = "\\n\\n".join(context_chunks)

            # í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ê³ ë“±í•™ìƒ ë©´ì ‘ ë§ì¶¤)
            prompt = f"""ë‹¹ì‹ ì€ ëŒ€í•™ ì…ì‹œ ë©´ì ‘ê´€ì…ë‹ˆë‹¤. í•™ìƒì˜ ë‹µë³€ì„ ë³´ê³  ë‹¤ìŒ ë‹¨ê³„ë¥¼ ê²°ì •í•˜ì„¸ìš”.

**ë©´ì ‘ ë‚œì´ë„**: {state['difficulty']}
**í˜„ì¬ ì£¼ì œ**: {state.get('current_sub_topic', 'ìê¸°ì†Œê°œ')}
**ë‚¨ì€ ì‹œê°„**: {state['remaining_time']}ì´ˆ

**ì´ì „ ì§ˆë¬¸**:
{last_question}

**í•™ìƒ ë‹µë³€** (ì†Œìš” ì‹œê°„: {response_time}ì´ˆ):
{user_answer}

**ê´€ë ¨ í•™ìƒë¶€ ì •ë³´**:
{context_text if context_text else "í•´ë‹¹ ì—†ìŒ"}

**ì¤‘ìš”: ì´ê²ƒì€ ê³ ë“±í•™ìƒ ëŒ€ìƒ ë©´ì ‘ì…ë‹ˆë‹¤**
- ê³ ë“±í•™ìƒ ìˆ˜ì¤€ì— ë§ê²Œ íŒë‹¨í•˜ì„¸ìš”
- ì ë‹¹íˆ ëŒ€ë‹µí•˜ë©´ ë°”ë¡œ ë‹¤ìŒ ì£¼ì œë¡œ ë„˜ì–´ê°€ì„¸ìš” (new_topic)
- í‰ê°€ëŠ” ë©´ì ‘ ì¢…ë£Œ í›„ì— í•©ë‹ˆë‹¤. ì¤‘ê°„ì— ë„ˆë¬´ ê¹Šê²Œ íŒŒì§€ ë§ˆì„¸ìš”
- ë„ˆë¬´ ì‹¤ë¬´ì ì¸ ì§ˆë¬¸ìœ¼ë¡œ ë“¤ì–´ê°€ì§€ ë§ˆì„¸ìš” (í˜„ì§ì ìˆ˜ì¤€ ì§ˆë¬¸ ê¸ˆì§€)
- ê¼¬ë¦¬ ì§ˆë¬¸ì€ ìµœëŒ€ 1íšŒë§Œ í•˜ì„¸ìš” (follow_up)

**ê²°ì • ê¸°ì¤€**:
   - follow_up: ë‹µë³€ì´ ë„ˆë¬´ ì¶”ìƒì ì´ê±°ë‚˜ ì´í•´ê°€ ì•ˆ ë  ë•Œë§Œ 1íšŒë§Œ (ì´í›„ì—ëŠ” ë¬´ì¡°ê±´ new_topic)
   - new_topic: ì ë‹¹íˆ ëŒ€ë‹µí–ˆê±°ë‚˜, ê¼¬ë¦¬ ì§ˆë¬¸ 1íšŒ í–ˆìœ¼ë©´ ë¬´ì¡°ê±´ ë‹¤ìŒ ì£¼ì œë¡œ
   - wrap_up: ì‹œê°„ì´ ë¶€ì¡±í•˜ê±°ë‚˜(30ì´ˆ ë¯¸ë§Œ) ë” ì´ìƒ ì§ˆë¬¸í•  ì£¼ì œê°€ ì—†ì„ ë•Œ

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”."""

            # JSON ìŠ¤í‚¤ë§ˆ (ê°„ì†Œí™” - reasoning ì œê±°)
            schema = self.types.Schema(
                type=self.types.Type.OBJECT,
                properties={
                    "action": self.types.Schema(type=self.types.Type.STRING, description="ë‹¤ìŒ ì•¡ì…˜ (follow_up, new_topic, wrap_up)")
                },
                required=["action"]
            )
            
            # Gemini í˜¸ì¶œ
            response = self.client.models.generate_content(
                model=self.model,
                contents=prompt,
                config={
                    "response_mime_type": "application/json",
                    "response_json_schema": schema,
                }
            )
            
            result = json.loads(response.text)

            # INTRO ë‹¨ê³„(ìê¸°ì†Œê°œ)ëŠ” ì´ë¯¸ initialize_interviewì—ì„œ ì €ì¥í–ˆìœ¼ë¯€ë¡œ ê±´ë„ˆëœ€
            if state.get('interview_stage') != 'INTRO':
                log_entry = {
                    "question": last_question,
                    "answer": user_answer,
                    "response_time": response_time,
                    "sub_topic": state.get('current_sub_topic', '')
                }
                self._save_interview_log(state, log_entry)

            # ë§ˆì§€ë§‰ ë‹µë³€ ì—…ë°ì´íŠ¸
            state['last_answer'] = user_answer

            # ë‹¤ìŒ ì•¡ì…˜ ì €ì¥
            state['next_action'] = result['action']

            logger.info(f"Analysis complete: {result['action']}")
            return state

        except Exception as e:
            import traceback
            logger.error(f"Error in analyzer: {e}")
            logger.error(f"Full traceback:\n{traceback.format_exc()}")
            state['next_action'] = "wrap_up"
            return state
        finally:
            if db:
                db.close()
    
    def decide_next_action(self, state: InterviewState) -> str:
        """ë‹¤ìŒ ì•¡ì…˜ ê²°ì • (Conditional Edge)"""
        return state.get('next_action', 'wrap_up')
    
    def retrieve_new_topic(
        self,
        state: InterviewState
    ) -> InterviewState:
        """ìƒˆë¡œìš´ ì£¼ì œ ê²€ìƒ‰"""
        db = None
        try:
            # ë¯¸ì¤‘ë³µ ì£¼ì œ ì„ íƒ
            remaining_topics = [
                topic for topic in SUB_TOPICS
                if topic not in state.get('asked_sub_topics', [])
            ]

            if not remaining_topics:
                logger.info("No more topics available")
                state['next_action'] = "wrap_up"
                return state

            # ëœë¤ ì„ íƒ (ë˜ëŠ” ì „ëµì  ì„ íƒ)
            import random
            new_topic = random.choice(remaining_topics)

            logger.info(f"Selected new topic: {new_topic}")

            # ë²¡í„° DBì—ì„œ ê´€ë ¨ ì²­í¬ ê²€ìƒ‰ (DB ì„¸ì…˜ ì¬ì‚¬ìš©)
            from app.services.vector_service import vector_service

            db = SessionLocal()
            chunks = vector_service.search_chunks_by_topic(
                record_id=state['record_id'],
                topic=new_topic,
                db=db  # DB ì„¸ì…˜ ì „ë‹¬
            )

            state['current_sub_topic'] = new_topic
            state['current_context'] = chunks  # ì´ë¯¸ text ë¦¬ìŠ¤íŠ¸
            state['asked_sub_topics'].append(new_topic)
            state['follow_up_count'] = 0

            return state

        except Exception as e:
            import traceback
            logger.error(f"Error retrieving new topic: {e}")
            logger.error(f"Full traceback:\n{traceback.format_exc()}")
            state['next_action'] = "wrap_up"
            return state
        finally:
            if db:
                db.close()
    
    def follow_up_generator(self, state: InterviewState) -> InterviewState:
        """ê¼¬ë¦¬ ì§ˆë¬¸ ìƒì„±"""
        try:
            logger.info(f"Generating follow-up question for: {state.get('current_sub_topic')}")

            # ë§ˆì§€ë§‰ ë‹µë³€ ê°€ì ¸ì˜¤ê¸° (stateì˜ last_answer ì‚¬ìš©)
            last_answer = state.get('last_answer', '')

            # ID ë¦¬ìŠ¤íŠ¸ë¡œ í…ìŠ¤íŠ¸ ì¡°íšŒ
            context_chunks = self._get_chunks_by_ids(state.get('current_context', []))
            context_text = "\\n\\n".join(context_chunks)

            # ê¼¬ë¦¬ ì§ˆë¬¸ í”„ë¡¬í”„íŠ¸
            prompt = f"""ë‹¹ì‹ ì€ ëŒ€í•™ ì…ì‹œ ë©´ì ‘ê´€ì…ë‹ˆë‹¤. í•™ìƒì˜ ë‹µë³€ì— ëŒ€í•´ ê¼¬ë¦¬ ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”.

**ë©´ì ‘ ë‚œì´ë„**: {state['difficulty']}
**í˜„ì¬ ì£¼ì œ**: {state.get('current_sub_topic')}
**ê¼¬ë¦¬ ì§ˆë¬¸ íšŸìˆ˜**: {state.get('follow_up_count', 0) + 1}íšŒì°¨

**ì´ì „ ë‹µë³€**:
{last_answer}

**ê´€ë ¨ í•™ìƒë¶€ ì •ë³´**:
{context_text}

**ê¼¬ë¦¬ ì§ˆë¬¸ ìƒì„± ì§€ì¹¨**:
1. ë‹µë³€ì—ì„œ ì–¸ê¸‰ëœ êµ¬ì²´ì  ì‚¬ë¡€, íŒë‹¨ ê·¼ê±°, ë°°ìš´ ì ì„ ì§‘ìš”í•˜ê²Œ ìºë¬»ìœ¼ì„¸ìš”.
2. "ì™œ ê·¸ë ‡ê²Œ ìƒê°í–ˆë‚˜?", "êµ¬ì²´ì ìœ¼ë¡œ ì–´ë–¤ ê²°ê³¼ì˜€ë‚˜?", "ê·¸ ê³¼ì •ì—ì„œ ì–´ë–¤ ê³ ë¯¼ì´ ìˆì—ˆë‚˜?" ë“±ì˜ íŒ¨í„´ í™œìš©
3. Hard ëª¨ë“œì—ì„œëŠ” ë…¼ë¦¬ì  í—ˆì ì„ ì°Œë¥´ëŠ” ì••ë°• ì§ˆë¬¸ ìƒì„±
4. í•™ìƒë¶€ ì •ë³´ì™€ êµì°¨ ê²€ì¦í•˜ì—¬ ì§ˆë¬¸

ë‹¤ìŒ ê¼¬ë¦¬ ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”."""

            # JSON ìŠ¤í‚¤ë§ˆ (context_summary ì œê±°)
            schema = self.types.Schema(
                type=self.types.Type.OBJECT,
                properties={
                    "question": self.types.Schema(type=self.types.Type.STRING)
                },
                required=["question"]
            )
            
            response = self.client.models.generate_content(
                model=self.model,
                contents=prompt,
                config=self.types.GenerateContentConfig(
                    response_mime_type="application/json",
                    response_schema=schema,
                    temperature=0.8
                )
            )

            result = json.loads(response.text)

            # ë§ˆì§€ë§‰ ì§ˆë¬¸ ì—…ë°ì´íŠ¸ (stateì—ë§Œ ì €ì¥, checkpoint ì•„ë‹˜)
            state['last_question'] = result['question']
            state['follow_up_count'] = state.get('follow_up_count', 0) + 1

            return state

        except Exception as e:
            error_msg = str(e)
            logger.error(f"Error generating follow-up question: {e}")

            # 429 í• ë‹¹ëŸ‰ ì´ˆê³¼ ì—ëŸ¬ ì²˜ë¦¬
            if "429" in error_msg or "RESOURCE_EXHAUSTED" in error_msg:
                state['error'] = "API í• ë‹¹ëŸ‰ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
                state['is_finished'] = True
                return state

            # ê·¸ ì™¸ ì—ëŸ¬
            state['error'] = f"ë©´ì ‘ ì§„í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error_msg}"
            state['is_finished'] = True
            return state
    
    def new_question_generator(self, state: InterviewState) -> InterviewState:
        """ìƒˆë¡œìš´ ì£¼ì œ ì²« ì§ˆë¬¸ ìƒì„±"""
        try:
            logger.info(f"Generating first question for topic: {state.get('current_sub_topic')}")

            # ID ë¦¬ìŠ¤íŠ¸ë¡œ í…ìŠ¤íŠ¸ ì¡°íšŒ
            context_chunks = self._get_chunks_by_ids(state.get('current_context', []))

            # ğŸ” ë””ë²„ê¹…: ì²­í¬ ë‚´ìš© ë¡œê·¸ ì¶œë ¥
            logger.info(f"ğŸ“š Retrieved {len(context_chunks)} chunks for topic '{state.get('current_sub_topic')}':")
            for i, chunk in enumerate(context_chunks):
                logger.info(f"  Chunk {i+1}: {chunk[:300]}...")  # ì²« 300ìë§Œ ì¶œë ¥

            context_text = "\n\n".join(context_chunks)

            # ì²« ì§ˆë¬¸ í”„ë¡¬í”„íŠ¸
            prompt = f"""ë‹¹ì‹ ì€ ëŒ€í•™ ì…ì‹œ ë©´ì ‘ê´€ì…ë‹ˆë‹¤. ìƒˆë¡œìš´ ì£¼ì œì— ëŒ€í•œ ì²« ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”.

**ë©´ì ‘ ë‚œì´ë„**: {state['difficulty']}
**ìƒˆë¡œìš´ ì£¼ì œ**: {state.get('current_sub_topic')}

**ê´€ë ¨ í•™ìƒë¶€ ì •ë³´**:
{context_text}

**ì²« ì§ˆë¬¸ ìƒì„± ì§€ì¹¨**:
1. í•´ë‹¹ ì£¼ì œì™€ ê´€ë ¨ëœ ê°œë°©í˜• ì§ˆë¬¸ ìƒì„±
2. í•™ìƒì˜ ê²½í—˜ê³¼ ìƒê°ì„ ììœ ë¡­ê²Œ í‘œí˜„í•˜ê²Œ ìœ ë„
3. êµ¬ì²´ì ì¸ ì‚¬ë¡€ë¥¼ ìš”ì²­í•˜ëŠ” ë°©ì‹

ì£¼ì œ ê°€ì´ë“œë¼ì¸:
- ì¶œê²°: ì§€ê°/ê²°ì„ íŒ¨í„´ê³¼ ì‚¬ìœ , ì„±ì‹¤ì„±
- ì„±ì : ì „ê³µ ê³¼ëª© ì„±ì  ì¶”ì´ì™€ ë³€í™” ì´ìœ 
- ë™ì•„ë¦¬: í”„ë¡œì íŠ¸ ë‚´ ì—­í• ê³¼ ê¸°ìˆ ì  í•´ê²° ê³¼ì •
- ë¦¬ë”ì‹­: ê°ˆë“± ìƒí™©ì—ì„œì˜ í•´ê²° ë©”ì»¤ë‹ˆì¦˜
- ì¸ì„±/íƒœë„: í–‰íŠ¹ ê¸°ë¡ ê¸°ë°˜ ë³¸ì¸ì˜ ëŒ€í‘œ íŠ¹ì„±
- ì§„ë¡œ/ììœ¨: ì§€ì› ì „ê³µ ê´€ì‹¬ ê³„ê¸°ì™€ í™œë™ ì—°ê²°
- ë…ì„œ: ë„ì„œê°€ ê°€ì¹˜ê´€ ë° íƒêµ¬ì— ë¯¸ì¹œ ì˜í–¥
- ë´‰ì‚¬: í™œë™ì˜ ì§€ì†ì„±ê³¼ ë°°ìš´ ì 

ì²« ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”."""

            # JSON ìŠ¤í‚¤ë§ˆ (context_summary ì œê±°)
            schema = self.types.Schema(
                type=self.types.Type.OBJECT,
                properties={
                    "question": self.types.Schema(type=self.types.Type.STRING)
                },
                required=["question"]
            )
            
            response = self.client.models.generate_content(
                model=self.model,
                contents=prompt,
                config={
    "response_mime_type": "application/json",
    "response_json_schema": schema,
}
            )

            result = json.loads(response.text)

            # ğŸ” ë””ë²„ê¹…: ìƒì„±ëœ ì§ˆë¬¸ ë¡œê·¸
            logger.info(f"âœ… Generated question: {result['question']}")

            # ë§ˆì§€ë§‰ ì§ˆë¬¸ ì—…ë°ì´íŠ¸ (stateì—ë§Œ ì €ì¥, checkpoint ì•„ë‹˜)
            state['last_question'] = result['question']

            return state

        except Exception as e:
            logger.error(f"Error generating new question: {e}")
            # ì—ëŸ¬ ë°œìƒ ì‹œ ë¹ˆ ì§ˆë¬¸ ë°˜í™˜
            return state
    
    def wrap_up(self, state: InterviewState) -> InterviewState:
        """ë©´ì ‘ ì¢…ë£Œ ë° ìš”ì•½ ìƒì„±"""
        db = None
        try:
            logger.info("Generating wrap-up summary")

            # InterviewSession ì—…ë°ì´íŠ¸ (ì¢…ë£Œ ìƒíƒœë§Œ, ë¡œê·¸ëŠ” ì´ë¯¸ incrementally ì €ì¥ë¨)
            session_id = state.get('session_id')
            total_questions = 0
            avg_response_time = 0

            if session_id:
                db = SessionLocal()
                try:
                    session = db.query(InterviewSession).filter(InterviewSession.id == session_id).first()
                    if session:
                        # interview_logsì—ì„œ í†µê³„ ê³„ì‚°
                        interview_logs = session.interview_logs or []
                        total_questions = len(interview_logs)

                        if interview_logs:
                            total_time = sum(log.get('response_time', 0) for log in interview_logs)
                            avg_response_time = total_time // total_questions

                        # ì„¸ì…˜ ì¢…ë£Œ ìƒíƒœë¡œ ì—…ë°ì´íŠ¸
                        session.status = "COMPLETED"
                        session.avg_response_time = avg_response_time
                        session.completed_at = func.now()
                        db.commit()
                        logger.info(f"Updated interview session {session_id} to COMPLETED with {total_questions} logs")
                finally:
                    db.close()

            # ê°„ë‹¨í•œ ì¢…ë£Œ ë©”ì‹œì§€ë§Œ ìƒì„± (ìƒì„¸ ë¶„ì„ì€ analyze_interview_resultì—ì„œ)
            closing_message = f"""ë©´ì ‘ì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤.

ğŸ“Š **ë©´ì ‘ ìš”ì•½**
- ì´ ì§ˆë¬¸ ìˆ˜: {total_questions}ê°œ
- ì†Œìš” ì‹œê°„: {600 - state.get('remaining_time', 600)}ì´ˆ

ìƒì„¸ ë¶„ì„ ê²°ê³¼ëŠ” ë©´ì ‘ ì¢…ë£Œ í›„ í™•ì¸í•´ì£¼ì„¸ìš”."""

            state['interview_stage'] = "WRAP_UP"

            return state

        except Exception as e:
            logger.error(f"Error in wrap_up: {e}")
            state['interview_stage'] = "WRAP_UP"
            return state
    

    def initialize_interview(
        self,
        user_id: int,
        record_id: int,
        difficulty: str,
        first_answer: str,
        response_time: int,
        thread_id: str
    ) -> Dict[str, Any]:
        """
        ë©´ì ‘ ì´ˆê¸°í™” (ì²« ë‹µë³€ ì²˜ë¦¬)

        Args:
            user_id: ì‚¬ìš©ì ID
            record_id: ìƒê¸°ë¶€ ID
            difficulty: ë‚œì´ë„ (Easy, Normal, Hard)
            first_answer: ì²« ë‹µë³€ (ìê¸°ì†Œê°œ)
            response_time: ë‹µë³€ ì†Œìš” ì‹œê°„
            thread_id: LangGraph thread ID

        Returns:
            Dict with next_question, updated_state, is_finished
        """
        db = None
        try:
            logger.info(f"Initializing interview for record {record_id}, difficulty: {difficulty}")

            # InterviewSession ìƒì„±
            db = SessionLocal()
            interview_session = InterviewSession(
                user_id=user_id,
                record_id=record_id,
                thread_id=thread_id,
                difficulty=difficulty,
                status="IN_PROGRESS",
                interview_logs=[{  # ì²« ë¡œê·¸ ì €ì¥
                    "question": "ìê¸°ì†Œê°œ ë¶€íƒë“œë¦½ë‹ˆë‹¤.",
                    "answer": first_answer,
                    "response_time": response_time,
                    "sub_topic": ""
                }]
            )
            db.add(interview_session)
            db.commit()
            db.refresh(interview_session)
            logger.info(f"Created interview session: {interview_session.id}")

            # ì´ˆê¸° ìƒíƒœ ìƒì„±
            initial_state: InterviewState = {
                'difficulty': difficulty,
                'remaining_time': 600,  # 10ë¶„
                'interview_stage': 'INTRO',
                'current_context': [],
                'current_sub_topic': '',
                'asked_sub_topics': [],
                'next_action': '',
                'follow_up_count': 0,
                'session_id': interview_session.id,  # ì„¸ì…˜ ID ì €ì¥
                'record_id': record_id,
                'current_user_answer': first_answer,
                'current_response_time': response_time,
                'last_question': 'ìê¸°ì†Œê°œ ë¶€íƒë“œë¦½ë‹ˆë‹¤.',
                'last_answer': ''
            }

            # process_answer ì¬ì‚¬ìš©
            return self.process_answer(
                state=initial_state,
                user_answer=first_answer,
                response_time=response_time,
                thread_id=thread_id
            )

        except Exception as e:
            logger.error(f"Error initializing interview: {e}")
            if db:
                db.rollback()
            raise
        finally:
            if db:
                db.close()

    def get_state(self, thread_id: str) -> InterviewState:
        """
        thread_idë¡œ í˜„ì¬ ìƒíƒœ ì¡°íšŒ

        Args:
            thread_id: LangGraph thread ID

        Returns:
            í˜„ì¬ InterviewState
        """
        try:
            config = {"configurable": {"thread_id": thread_id}}

            # PostgresSaver ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ë‚´ì—ì„œ ìƒíƒœ ì¡°íšŒ
            with PostgresSaver.from_conn_string(self._conn_string) as checkpointer:
                # get_tupleë¡œ ì „ì²´ íŠœí”Œ ê°€ì ¸ì˜¤ê¸°
                result = checkpointer.get_tuple(config=config)

                if result is None:
                    raise ValueError(f"No state found for thread_id: {thread_id}")

                # result.checkpoint['channel_values']ì— ìš°ë¦¬ InterviewState ë°ì´í„°ê°€ ìˆìŒ
                return result.checkpoint['channel_values']

        except Exception as e:
            import traceback
            logger.error(f"Error getting state for thread_id {thread_id}: {e}")
            logger.error(f"Full traceback:\n{traceback.format_exc()}")
            raise

    def process_answer(
        self,
        state: InterviewState,
        user_answer: str,
        response_time: int,
        thread_id: str
    ) -> str:
        """
        ë‹µë³€ ì²˜ë¦¬ ë° ë‹¤ìŒ ì§ˆë¬¸ ìƒì„± (LangGraph invoke ë°©ì‹)

        Args:
            state: í˜„ì¬ ë©´ì ‘ ìƒíƒœ (record_id í¬í•¨)
            user_answer: ì‚¬ìš©ì ë‹µë³€
            response_time: ë‹µë³€ ì†Œìš” ì‹œê°„
            thread_id: LangGraph thread ID (Checkpointerìš©)

        Returns:
            str: ë‹¤ìŒ ì§ˆë¬¸ í…ìŠ¤íŠ¸
        """
        try:
            # í˜„ì¬ ë‹µë³€ ì •ë³´ë§Œ stateì— ì„¤ì • (pass-through í•„ë“œ)
            state['current_user_answer'] = user_answer
            state['current_response_time'] = response_time

            # ë‚¨ì€ ì‹œê°„ ì²´í¬
            if state['remaining_time'] < 30:
                state['next_action'] = "wrap_up"

            # PostgresSaver ì»¨í…ìŠ¤íŠ¸ ë‚´ì—ì„œ ê·¸ë˜í”„ ì‹¤í–‰
            with PostgresSaver.from_conn_string(self._conn_string) as checkpointer:
                graph = self._build_workflow().compile(checkpointer=checkpointer)
                config = {"configurable": {"thread_id": thread_id}}
                result_state = graph.invoke(state, config=config)

                # last_questionì—ì„œ ë‹¤ìŒ ì§ˆë¬¸ ì¶”ì¶œ
                next_question = result_state.get('last_question', '')

                return next_question

        except Exception as e:
            import traceback
            error_msg = str(e)
            logger.error(f"Error processing answer: {error_msg}")
            logger.error(f"Full traceback:\n{traceback.format_exc()}")

            # 429 í• ë‹¹ëŸ‰ ì´ˆê³¼ ì—ëŸ¬
            if "429" in error_msg or "RESOURCE_EXHAUSTED" in error_msg:
                raise Exception("API_QUOTA_EXCEEDED: Google Gemini API í• ë‹¹ëŸ‰ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")

            # ê·¸ ì™¸ ì—ëŸ¬
            raise Exception(f"ë©´ì ‘ ì§„í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error_msg}")

    def analyze_interview_result(self, thread_id: str) -> Dict[str, Any]:
        """
        ë©´ì ‘ ê²°ê³¼ ë¶„ì„ ë° ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±

        Args:
            thread_id: LangGraph thread ID

        Returns:
            ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸
        """
        db = None
        try:
            logger.info(f"Analyzing interview result for thread_id: {thread_id}")

            # 1. DBì—ì„œ ì§ì ‘ InterviewSession ì¡°íšŒ (checkpoint ì‚¬ìš© ì•ˆ í•¨)
            db = SessionLocal()
            interview_session = db.query(InterviewSession).filter(
                InterviewSession.thread_id == thread_id
            ).first()

            if not interview_session:
                return {
                    "error": "No interview data found",
                    "message": "ë©´ì ‘ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
                }

            # 2. InterviewSessionì—ì„œ ë°ì´í„° ì¶”ì¶œ
            answer_log = interview_session.interview_logs if interview_session.interview_logs else []
            difficulty = interview_session.difficulty
            avg_response_time = interview_session.avg_response_time or 0
            total_duration = interview_session.total_duration or 0

            if not answer_log:
                return {
                    "error": "No interview data found",
                    "message": "ë©´ì ‘ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
                }

            logger.info(f"Found {len(answer_log)} logs from DB for analysis")

            # 3. ëŒ€í™” ìš”ì•½ ìƒì„± (ì „ì²´ ë‹µë³€ ì‚¬ìš© - ë‹µë³€ ê¸¸ì´ ì œí•œ)
            conversation_summary = []
            for log in answer_log:
                # ë‹µë³€ì´ ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°
                answer = log.get('answer', '')
                if len(answer) > 500:
                    answer = answer[:500] + "... (ìƒëµ)"

                conversation_summary.append(f"Q: {log.get('question', '')}")
                conversation_summary.append(f"A: {answer} (ì†Œìš”ì‹œê°„: {log.get('response_time', 0)}ì´ˆ)")

            summary_text = "\n".join(conversation_summary)

            # 4. AI ë¶„ì„ í”„ë¡¬í”„íŠ¸ (ë‹µë³€ ê¸¸ì´ ì œí•œë¨)
            prompt = f"""ë‹¹ì‹ ì€ ëŒ€í•™ ì…ì‹œ ë©´ì ‘ê´€ì…ë‹ˆë‹¤. ë©´ì ‘ ì¢…ë£Œ í›„ ì¢…í•© í‰ê°€ë¥¼ ìƒì„±í•˜ì„¸ìš”.

**ë©´ì ‘ ë‚œì´ë„**: {difficulty}
**ì´ ë‹µë³€ ìˆ˜**: {len(answer_log)}
**í‰ê·  ì‘ë‹µ ì‹œê°„**: {avg_response_time}ì´ˆ

**ì „ì²´ ëŒ€í™” ë‚´ìš©** (ë‹µë³€ì€ 500ìë¡œ ìš”ì•½ë¨):
{summary_text}

**ì ìˆ˜ ì‚°ì • ê¸°ì¤€**:
- ì „ê³µì í•©ì„±: 0~25ì  (ì§€ì› ì „ê³µì— ëŒ€í•œ ì´í•´ë„, ê´€ë ¨ í™œë™ê³¼ì˜ ì—°ê²°ì„±)
- ì¸ì„±: 0~25ì  (íƒœë„, ì„±ì‹¤ì„±, íƒ€ì¸ì— ëŒ€í•œ ë°°ë ¤)
- ë°œì „ê°€ëŠ¥ì„±: 0~25ì  (í•™ìŠµ ì˜ì§€, ì„±ì¥ ë§ˆì¸ë“œ, ìê¸° ê°œì„  ë…¸ë ¥)
- ì˜ì‚¬ì†Œí†µëŠ¥ë ¥: 0~25ì  (ë…¼ë¦¬ì  ë§í•˜ê¸°, ëª…í™•í•œ í‘œí˜„, ê²½ì²­ íƒœë„)
- ì´ì : 0~100ì  (ìœ„ 4ê°œ ì˜ì—­ í•©ê³„)

**ê°•ì  íƒœê·¸ ì˜ˆì‹œ**: êµ¬ì²´ì  ì‚¬ë¡€ ì œì‹œ, ë…¼ë¦¬ì  êµ¬ì¡°ë¥¼ ê°€ì§, ìì‹ ê° ìˆëŠ” íƒœë„, êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ ì¸ìš©, ì„±ì‹¤í•œ ë‹µë³€ ë“±

**ë‹¨ì  íƒœê·¸ ì˜ˆì‹œ**: ë‹µë³€ ì‹œê°„ì´ ëŠë¦¼, ê·¼ê±° ë¶€ì¡±, ì§ˆë¬¸ ì˜ë„ ì¬í™•ì¸ í•„ìš”, ì¶”ìƒì ì¸ ë‹µë³€, ê²°ë¡ ì´ ë¶ˆëª…í™•í•¨ ë“±

**ìƒì„¸ ë¶„ì„ ê¸°ì¤€**:
- í‰ê°€: ì¢‹ìŒ/ë³´í†µ/ë‚˜ì¨ (ë‹µë³€ì˜ ì¶©ì‹¤ë„, êµ¬ì²´ì„±, ë…¼ë¦¬ì„± ê³ ë ¤)
- ê°œì„  í¬ì¸íŠ¸: "ë‚´ ì—­í• ì„ ë” ëª…í™•íˆ ê°•ì¡°í•˜ê¸°", "ê²°ë¡ ì„ ë¨¼ì € ë§í•˜ê³  êµ¬ì²´ ì‚¬ë¡€ ë§ë¶™ì´ê¸°" ë“±
- ë³´ì™„ í•„ìš”: "ë°°ìš´ ì ì„ ì „ê³µê³¼ ì—°ê²°í•˜ëŠ” ë¬¸ì¥ 1ì¤„ ì¶”ê°€", "êµ¬ì²´ì ì¸ ê²°ê³¼ ìˆ˜ì¹˜ ì–¸ê¸‰í•˜ê¸°" ë“±

**JSON í˜•ì‹ìœ¼ë¡œ ì¢…í•© í‰ê°€ë¥¼ ìƒì„±í•˜ì„¸ìš”.**

ê° ë‹µë³€ì— ëŒ€í•´ ì§ˆë¬¸ ë‚´ìš©, ë‹µë³€ ì‹œê°„, í‰ê°€, ê°œì„  í¬ì¸íŠ¸, ë³´ì™„ í•„ìš” í•­ëª©ì„ ë¶„ì„í•˜ì„¸ìš”."""

            # 5. JSON ìŠ¤í‚¤ë§ˆ
            schema = self.types.Schema(
                type=self.types.Type.OBJECT,
                properties={
                    "scores": self.types.Schema(
                        type=self.types.Type.OBJECT,
                        properties={
                            "ì „ê³µì í•©ì„±": self.types.Schema(type=self.types.Type.INTEGER, minimum=0, maximum=25),
                            "ì¸ì„±": self.types.Schema(type=self.types.Type.INTEGER, minimum=0, maximum=25),
                            "ë°œì „ê°€ëŠ¥ì„±": self.types.Schema(type=self.types.Type.INTEGER, minimum=0, maximum=25),
                            "ì˜ì‚¬ì†Œí†µëŠ¥ë ¥": self.types.Schema(type=self.types.Type.INTEGER, minimum=0, maximum=25),
                            "ì´ì ": self.types.Schema(type=self.types.Type.INTEGER, minimum=0, maximum=100)
                        },
                        required=["ì „ê³µì í•©ì„±", "ì¸ì„±", "ë°œì „ê°€ëŠ¥ì„±", "ì˜ì‚¬ì†Œí†µëŠ¥ë ¥", "ì´ì "]
                    ),
                    "strength_tags": self.types.Schema(
                        type=self.types.Type.ARRAY,
                        items=self.types.Schema(type=self.types.Type.STRING)
                    ),
                    "weakness_tags": self.types.Schema(
                        type=self.types.Type.ARRAY,
                        items=self.types.Schema(type=self.types.Type.STRING)
                    ),
                    "detailed_analysis": self.types.Schema(
                        type=self.types.Type.ARRAY,
                        items=self.types.Schema(
                            type=self.types.Type.OBJECT,
                            properties={
                                "question": self.types.Schema(type=self.types.Type.STRING, description="ì§ˆë¬¸ ë‚´ìš©"),
                                "response_time": self.types.Schema(type=self.types.Type.INTEGER, description="ë‹µë³€ ì‹œê°„(ì´ˆ)"),
                                "evaluation": self.types.Schema(type=self.types.Type.STRING, description="í‰ê°€ (ì¢‹ìŒ/ë³´í†µ/ë‚˜ì¨)"),
                                "improvement_point": self.types.Schema(type=self.types.Type.STRING, description="ê°œì„  í¬ì¸íŠ¸"),
                                "supplement_needed": self.types.Schema(type=self.types.Type.STRING, description="ë³´ì™„ í•„ìš” ì‚¬í•­")
                            },
                            required=["question", "response_time", "evaluation", "improvement_point", "supplement_needed"]
                        )
                    )
                },
                required=["scores", "strength_tags", "weakness_tags", "detailed_analysis"]
            )

            # 8. Gemini í˜¸ì¶œ
            response = self.client.models.generate_content(
                model=self.model,
                contents=prompt,
                config={
                    "response_mime_type": "application/json",
                    "response_json_schema": schema,
                }
            )

            result = json.loads(response.text)

            # 6. InterviewSession ì—…ë°ì´íŠ¸ (ê°™ì€ db ì‚¬ìš©)
            interview_session.status = "COMPLETED"
            interview_session.completed_at = func.now()
            interview_session.avg_response_time = avg_response_time
            interview_session.total_questions = len(answer_log)
            interview_session.total_duration = total_duration
            interview_session.final_report = result
            db.commit()
            logger.info(f"Updated interview session {interview_session.id} to COMPLETED")

            # 7. ê²°ê³¼ ë°˜í™˜
            return {
                "scores": result.get("scores", {}),
                "strength_tags": result.get("strength_tags", []),
                "weakness_tags": result.get("weakness_tags", []),
                "detailed_analysis": result.get("detailed_analysis", [])
            }

        except Exception as e:
            logger.error(f"Error analyzing interview result: {e}")
            if db:
                db.rollback()
            return {
                "error": str(e),
                "message": "ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            }
        finally:
            if db:
                db.close()

    def _get_chunks_by_ids(self, chunk_ids: List[int]) -> List[str]:
        """ì²­í¬ ID ë¦¬ìŠ¤íŠ¸ë¡œ í…ìŠ¤íŠ¸ ì¡°íšŒ

        Args:
            chunk_ids: ì²­í¬ ID ë¦¬ìŠ¤íŠ¸

        Returns:
            ì²­í¬ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        """
        if not chunk_ids:
            return []

        db = None
        try:
            db = SessionLocal()
            from app.models import RecordChunk

            chunks = db.query(RecordChunk.chunk_text).filter(
                RecordChunk.id.in_(chunk_ids)
            ).all()

            return [chunk[0] for chunk in chunks]

        except Exception as e:
            logger.error(f"Error getting chunks by ids: {e}")
            return []
        finally:
            if db:
                db.close()

    def _save_interview_log(self, state: InterviewState, log_entry: Dict[str, Any]):
        """InterviewSessionì— ëŒ€í™” ë¡œê·¸ ì €ì¥

        Args:
            state: í˜„ì¬ ë©´ì ‘ ìƒíƒœ
            log_entry: ì €ì¥í•  ë¡œê·¸ ì—”íŠ¸ë¦¬
        """
        import json
        from sqlalchemy import text

        db = None
        try:
            db = SessionLocal()
            session_id = state.get('session_id')

            logger.info(f"ğŸ” _save_interview_log called: session_id={session_id}, question={log_entry.get('question', '')[:30]}")

            if not session_id:
                logger.error(f"âŒ session_id is missing in state! Available keys: {list(state.keys())}")
                return

            # í–‰ ì ê¸ˆ ì—†ì´ ì¡°íšŒ
            interview_session = db.query(InterviewSession).filter(
                InterviewSession.id == session_id
            ).first()

            if not interview_session:
                logger.error(f"âŒ InterviewSession not found for session_id: {session_id}")
                return

            # ê¸°ì¡´ ë¡œê·¸ ê°€ì ¸ì˜¤ê¸°
            logs = list(interview_session.interview_logs) if interview_session.interview_logs else []
            logs.append(log_entry)

            # RAW SQLë¡œ ì§ì ‘ ì—…ë°ì´íŠ¸ (SQLAlchemy ORM ìš°íšŒ)
            # CAST í•¨ìˆ˜ ì‚¬ìš© (:: ìºìŠ¤íŒ…ì€ SQLAlchemy íŒŒë¼ë¯¸í„°ì™€ ì¶©ëŒ)
            db.execute(text("""
                UPDATE interview_sessions
                SET interview_logs = CAST(:logs AS JSON)
                WHERE id = :session_id
            """), {"logs": json.dumps(logs, ensure_ascii=False), "session_id": session_id})

            # ì¦‰ì‹œ ì»¤ë°‹
            db.commit()

            logger.info(f"âœ… Saved log to interview_session {session_id} (total logs: {len(logs)})")

        except Exception as e:
            logger.error(f"âŒ Error saving interview log: {e}")
            import traceback
            logger.error(f"Full traceback:\n{traceback.format_exc()}")
            if db:
                db.rollback()
        finally:
            if db:
                db.close()


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
interview_graph = InterviewGraph()
