"""ì‹¤ì‹œê°„ ë©´ì ‘ LangGraph êµ¬í˜„

ê¼¬ë¦¬ ì§ˆë¬¸(Tail Questions) ì‹œìŠ¤í…œì„ í†µí•´ ì‹¬ì¸µì ì¸ ë©´ì ‘ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
ìƒíƒœ ì €ì¥ì€ LangGraphì˜ AsyncPostgresSaver Checkpointerê°€ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
"""
from typing import TypedDict, List, Dict, Any, Optional, Annotated
from operator import add
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.postgres import PostgresSaver
from pydantic import BaseModel, Field
from google import genai
from google.genai import types
from config import settings
from app.database import SessionLocal
from app.models import InterviewSession
from sqlalchemy.sql import func
import logging
import json

logger = logging.getLogger(__name__)


# ==================== Reducer í•¨ìˆ˜ ì •ì˜ ====================

def merge_logs(existing: List[Dict], new: List[Dict]) -> List[Dict]:
    """answer_log ë³‘í•© í•¨ìˆ˜"""
    if not existing:
        return new
    if not new:
        return existing
    return existing + new


# ==================== State ì •ì˜ ====================

class InterviewState(TypedDict):
    """ë©´ì ‘ ìƒíƒœ"""

    # ê¸°ë³¸ ì„¤ì •
    difficulty: str                    # ë©´ì ‘ ë‚œì´ë„ (Easy, Normal, Hard)
    remaining_time: int                # ë‚¨ì€ ì‹œê°„ (ì´ˆ ë‹¨ìœ„)
    interview_stage: str               # [INTRO, MAIN, WRAP_UP]

    # ëŒ€í™” ì»¨í…ìŠ¤íŠ¸
    current_context: List[str]         # í˜„ì¬ ì§ˆë¬¸/ì£¼ì œì™€ ê´€ë ¨ëœ í•™ìƒë¶€ ì²­í¬ ë¦¬ìŠ¤íŠ¸
    current_sub_topic: str             # í˜„ì¬ ì§„í–‰ ì¤‘ì¸ ì„¸ë¶€ ì£¼ì œ
    asked_sub_topics: List[str]        # ì´ë¯¸ ì™„ë£Œëœ ì„¸ë¶€ ì£¼ì œ ë¦¬ìŠ¤íŠ¸

    # ë‹µë³€ ê¸°ë¡ (checkpointì— ì €ì¥)
    answer_log: Annotated[List[Dict], merge_logs]

    # ë‚´ë¶€ ìƒíƒœ
    next_action: str                   # [follow_up, new_topic, wrap_up]
    follow_up_count: int               # í˜„ì¬ ì£¼ì œì— ëŒ€í•œ ê¼¬ë¦¬ ì§ˆë¬¸ íšŸìˆ˜

    # ì„¸ì…˜ ì •ë³´
    session_id: int                    # InterviewSession ID (ë°ì´í„°ë² ì´ìŠ¤ ì™¸ë˜í‚¤)
    record_id: int                     # ìƒê¸°ë¶€ ID

    # í˜„ì¬ ì²˜ë¦¬ ì¤‘ì¸ ë‹µë³€ (checkpointì— ì €ì¥í•˜ì§€ ì•ŠìŒ - pass through)
    current_user_answer: str
    current_response_time: int


# ==================== Pydantic ëª¨ë¸ ====================

class AnalyzerDecision(BaseModel):
    """ë¶„ì„ê¸° ê²°ì • ëª¨ë¸ - ê¼¬ë¦¬ì§ˆë¬¸ ì—¬ë¶€ë§Œ íŒë‹¨"""
    action: str = Field(description="ë‹¤ìŒ ì•¡ì…˜ (follow_up, new_topic, wrap_up)")
    reasoning: str = Field(description="ê²°ì • ê·¼ê±°")


class GeneratedQuestion(BaseModel):
    """ìƒì„±ëœ ì§ˆë¬¸ ëª¨ë¸"""
    question: str = Field(description="ì§ˆë¬¸ ë‚´ìš©")
    context_summary: str = Field(description="ì‚¬ìš©ëœ ì»¨í…ìŠ¤íŠ¸ ìš”ì•½")


# ==================== í•˜ìœ„ ì£¼ì œ ì •ì˜ ====================

SUB_TOPICS = [
    "ì¶œê²°", "ì„±ì ", "ë™ì•„ë¦¬", "ë¦¬ë”ì‹­", 
    "ì¸ì„±/íƒœë„", "ì§„ë¡œ/ììœ¨", "ë…ì„œ", "ë´‰ì‚¬"
]


# ==================== Interview Graph ====================

class InterviewGraph:
    """ì‹¤ì‹œê°„ ë©´ì ‘ LangGraph"""

    def __init__(self):
        # Google GenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.client = genai.Client(api_key=settings.google_api_key)
        self.model = "gemini-2.5-flash"  # Free Tier ë¬´ì œí•œ (LiteëŠ” í•˜ë£¨ 20íšŒ ì œí•œ)
        self.types = types

        # database_url ì €ì¥ (PostgresSaverìš©)
        # postgresql+psycopg2:// â†’ postgresql://
        # postgresql+psycopg:// â†’ postgresql://
        self._conn_string = settings.database_url
        self._conn_string = self._conn_string.replace("postgresql+psycopg2://", "postgresql://", 1)
        self._conn_string = self._conn_string.replace("postgresql+psycopg://", "postgresql://", 1)

        self._graph = None

    def get_graph(self):
        """ê·¸ë˜í”„ ë°˜í™˜ (checkpointer ì—†ì´ ì»´íŒŒì¼)"""
        if self._graph is None:
            # ê·¸ë˜í”„ ë¹Œë“œ ë° ì»´íŒŒì¼ (checkpointer ì—†ì´)
            self._graph = self._build_workflow().compile()

        return self._graph

    def _build_workflow(self) -> StateGraph:
        """Workflow ë¹Œë“œ (ì»´íŒŒì¼ ì „)"""
        workflow = StateGraph(InterviewState)

        # ë…¸ë“œ ì¶”ê°€
        workflow.add_node("analyzer", self.analyzer)
        workflow.add_node("retrieve_new_topic", self.retrieve_new_topic)
        workflow.add_node("follow_up_generator", self.follow_up_generator)
        workflow.add_node("new_question_generator", self.new_question_generator)
        workflow.add_node("wrap_up", self.wrap_up)

        # ì—”íŠ¸ë¦¬ í¬ì¸íŠ¸
        workflow.set_entry_point("analyzer")

        # ì¡°ê±´ë¶€ ì—£ì§€
        workflow.add_conditional_edges(
            "analyzer",
            self.decide_next_action,
            {
                "follow_up": "follow_up_generator",
                "new_topic": "retrieve_new_topic",
                "wrap_up": "wrap_up"
            }
        )

        # ì¼ë°˜ ì—£ì§€
        workflow.add_edge("retrieve_new_topic", "new_question_generator")
        workflow.add_edge("follow_up_generator", END)
        workflow.add_edge("new_question_generator", END)
        workflow.add_edge("wrap_up", END)

        return workflow

    def analyzer(self, state: InterviewState) -> InterviewState:
        """ë‹µë³€ ë¶„ì„ ë° ë‹¤ìŒ ì•¡ì…˜ ê²°ì • (ê¼¬ë¦¬ì§ˆë¬¸ ì—¬ë¶€ë§Œ íŒë‹¨)"""
        try:
            logger.info(f"Analyzing answer for topic: {state.get('current_sub_topic', 'INTRO')}")

            # í˜„ì¬ ë‹µë³€ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            user_answer = state.get('current_user_answer', '')
            response_time = state.get('current_response_time', 0)

            # ë§ˆì§€ë§‰ ì§ˆë¬¸ ê°€ì ¸ì˜¤ê¸° (answer_logì—ì„œ)
            last_question = ""
            answer_log = state.get('answer_log', [])
            if answer_log:
                last_question = answer_log[-1].get('question', '')

            # ì»¨í…ìŠ¤íŠ¸ í…ìŠ¤íŠ¸ êµ¬ì„±
            context_text = "\\n\\n".join(state.get('current_context', []))

            # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            prompt = f"""ë‹¹ì‹ ì€ ëŒ€í•™ ì…ì‹œ ë©´ì ‘ê´€ì…ë‹ˆë‹¤. í•™ìƒì˜ ë‹µë³€ì„ ë¶„ì„í•˜ê³  ë‹¤ìŒ ë‹¨ê³„ë¥¼ ê²°ì •í•˜ì„¸ìš”.

**ë©´ì ‘ ë‚œì´ë„**: {state['difficulty']}
**í˜„ì¬ ì£¼ì œ**: {state.get('current_sub_topic', 'ìê¸°ì†Œê°œ')}
**ë‚¨ì€ ì‹œê°„**: {state['remaining_time']}ì´ˆ

**ì´ì „ ì§ˆë¬¸**:
{last_question}

**í•™ìƒ ë‹µë³€** (ì†Œìš” ì‹œê°„: {response_time}ì´ˆ):
{user_answer}

**ê´€ë ¨ í•™ìƒë¶€ ì •ë³´**:
{context_text if context_text else "í•´ë‹¹ ì—†ìŒ"}

**ë¶„ì„ ì§€ì¹¨**:
ë‹¤ìŒ ì•¡ì…˜ì„ ê²°ì •í•˜ì„¸ìš”:
   - follow_up: ë‹µë³€ì´ ë¶ˆì¶©ë¶„í•˜ê±°ë‚˜ ë” ê¹Šì€ íŒŒê¸°ê°€ í•„ìš”í•  ë•Œ (êµ¬ì²´ì  ì‚¬ë¡€ ë¶€ì¡±, ë…¼ë¦¬ì  í—ˆì , íŒë‹¨ ê·¼ê±° ë¶ˆëª…í™• ë“±)
   - new_topic: ë‹µë³€ì´ ì¶©ì‹¤í•˜ê³  êµ¬ì²´ì ì´ì–´ì„œ ì£¼ì œë¥¼ ë°”ê¿€ ë•Œ
   - wrap_up: ì‹œê°„ì´ ë¶€ì¡±í•˜ê±°ë‚˜(30ì´ˆ ë¯¸ë§Œ) ë” ì´ìƒ ì§ˆë¬¸í•  ì£¼ì œê°€ ì—†ì„ ë•Œ

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”."""

            # JSON ìŠ¤í‚¤ë§ˆ (ê°„ì†Œí™”)
            schema = self.types.Schema(
                type=self.types.Type.OBJECT,
                properties={
                    "action": self.types.Schema(type=self.types.Type.STRING, description="ë‹¤ìŒ ì•¡ì…˜ (follow_up, new_topic, wrap_up)"),
                    "reasoning": self.types.Schema(type=self.types.Type.STRING, description="ê²°ì • ê·¼ê±°")
                },
                required=["action", "reasoning"]
            )
            
            # Gemini í˜¸ì¶œ
            response = self.client.models.generate_content(
                model=self.model,
                contents=prompt,
                config={
                    "response_mime_type": "application/json",
                    "response_json_schema": schema,
                }
            )
            
            result = json.loads(response.text)
            
            # ë‹µë³€ ë¡œê·¸ ì €ì¥
            from datetime import datetime
            log_entry = {
                "question": last_question,
                "answer": user_answer,
                "response_time": response_time,
                "sub_topic": state.get('current_sub_topic', ''),
                "timestamp": datetime.now().isoformat()
            }

            # ë¦¬ìŠ¤íŠ¸ì— ìƒˆ í•­ëª© ì¶”ê°€ (ìƒˆ ë¦¬ìŠ¤íŠ¸ ìƒì„±)
            state['answer_log'] = state.get('answer_log', []) + [log_entry]

            # ë‹¤ìŒ ì•¡ì…˜ ì €ì¥
            state['next_action'] = result['action']
            
            logger.info(f"Analysis complete: {result['action']} - {result['reasoning']}")
            return state
            
        except Exception as e:
            logger.error(f"Error in analyzer: {e}")
            state['next_action'] = "wrap_up"
            return state
    
    def decide_next_action(self, state: InterviewState) -> str:
        """ë‹¤ìŒ ì•¡ì…˜ ê²°ì • (Conditional Edge)"""
        return state.get('next_action', 'wrap_up')
    
    def retrieve_new_topic(
        self, 
        state: InterviewState
    ) -> InterviewState:
        """ìƒˆë¡œìš´ ì£¼ì œ ê²€ìƒ‰"""
        try:
            # ë¯¸ì¤‘ë³µ ì£¼ì œ ì„ íƒ
            remaining_topics = [
                topic for topic in SUB_TOPICS 
                if topic not in state.get('asked_sub_topics', [])
            ]
            
            if not remaining_topics:
                logger.info("No more topics available")
                state['next_action'] = "wrap_up"
                return state
            
            # ëœë¤ ì„ íƒ (ë˜ëŠ” ì „ëµì  ì„ íƒ)
            import random
            new_topic = random.choice(remaining_topics)
            
            logger.info(f"Selected new topic: {new_topic}")
            
            # ë²¡í„° DBì—ì„œ ê´€ë ¨ ì²­í¬ ê²€ìƒ‰
            from app.services.vector_service import vector_service
            
            chunks = vector_service.search_chunks_by_topic(
                record_id=state['record_id'],
                topic=new_topic
            )
            
            state['current_sub_topic'] = new_topic
            state['current_context'] = chunks  # ì´ë¯¸ text ë¦¬ìŠ¤íŠ¸
            state['asked_sub_topics'].append(new_topic)
            state['follow_up_count'] = 0
            
            return state
            
        except Exception as e:
            logger.error(f"Error retrieving new topic: {e}")
            state['next_action'] = "wrap_up"
            return state
    
    def follow_up_generator(self, state: InterviewState) -> InterviewState:
        """ê¼¬ë¦¬ ì§ˆë¬¸ ìƒì„±"""
        try:
            logger.info(f"Generating follow-up question for: {state.get('current_sub_topic')}")

            # ë§ˆì§€ë§‰ ë‹µë³€ ê°€ì ¸ì˜¤ê¸° (answer_logì—ì„œ)
            last_answer = ""
            answer_log = state.get('answer_log', [])
            if answer_log:
                last_answer = answer_log[-1].get('answer', '')

            context_text = "\n\n".join(state.get('current_context', []))
            
            # ê¼¬ë¦¬ ì§ˆë¬¸ í”„ë¡¬í”„íŠ¸
            prompt = f"""ë‹¹ì‹ ì€ ëŒ€í•™ ì…ì‹œ ë©´ì ‘ê´€ì…ë‹ˆë‹¤. í•™ìƒì˜ ë‹µë³€ì— ëŒ€í•´ ê¼¬ë¦¬ ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”.

**ë©´ì ‘ ë‚œì´ë„**: {state['difficulty']}
**í˜„ì¬ ì£¼ì œ**: {state.get('current_sub_topic')}
**ê¼¬ë¦¬ ì§ˆë¬¸ íšŸìˆ˜**: {state.get('follow_up_count', 0) + 1}íšŒì°¨

**ì´ì „ ë‹µë³€**:
{last_answer}

**ê´€ë ¨ í•™ìƒë¶€ ì •ë³´**:
{context_text}

**ê¼¬ë¦¬ ì§ˆë¬¸ ìƒì„± ì§€ì¹¨**:
1. ë‹µë³€ì—ì„œ ì–¸ê¸‰ëœ êµ¬ì²´ì  ì‚¬ë¡€, íŒë‹¨ ê·¼ê±°, ë°°ìš´ ì ì„ ì§‘ìš”í•˜ê²Œ ìºë¬»ìœ¼ì„¸ìš”.
2. "ì™œ ê·¸ë ‡ê²Œ ìƒê°í–ˆë‚˜?", "êµ¬ì²´ì ìœ¼ë¡œ ì–´ë–¤ ê²°ê³¼ì˜€ë‚˜?", "ê·¸ ê³¼ì •ì—ì„œ ì–´ë–¤ ê³ ë¯¼ì´ ìˆì—ˆë‚˜?" ë“±ì˜ íŒ¨í„´ í™œìš©
3. Hard ëª¨ë“œì—ì„œëŠ” ë…¼ë¦¬ì  í—ˆì ì„ ì°Œë¥´ëŠ” ì••ë°• ì§ˆë¬¸ ìƒì„±
4. í•™ìƒë¶€ ì •ë³´ì™€ êµì°¨ ê²€ì¦í•˜ì—¬ ì§ˆë¬¸

ë‹¤ìŒ ê¼¬ë¦¬ ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”."""

            # JSON ìŠ¤í‚¤ë§ˆ
            schema = self.types.Schema(
                type=self.types.Type.OBJECT,
                properties={
                    "question": self.types.Schema(type=self.types.Type.STRING),
                    "context_summary": self.types.Schema(type=self.types.Type.STRING)
                },
                required=["question", "context_summary"]
            )
            
            response = self.client.models.generate_content(
                model=self.model,
                contents=prompt,
                config=self.types.GenerateContentConfig(
                    response_mime_type="application/json",
                    response_schema=schema,
                    temperature=0.8
                )
            )
            
            result = json.loads(response.text)

            # ìƒì„±ëœ ì§ˆë¬¸ì„ answer_logì— ì¶”ê°€
            from datetime import datetime
            log_entry = {
                "question": result['question'],
                "answer": "",
                "response_time": 0,
                "sub_topic": state.get('current_sub_topic', ''),
                "timestamp": datetime.now().isoformat(),
                "generated_question": result['question']
            }
            state['answer_log'] = state.get('answer_log', []) + [log_entry]
            state['follow_up_count'] = state.get('follow_up_count', 0) + 1

            return state

        except Exception as e:
            error_msg = str(e)
            logger.error(f"Error generating follow-up question: {e}")

            # 429 í• ë‹¹ëŸ‰ ì´ˆê³¼ ì—ëŸ¬ ì²˜ë¦¬
            if "429" in error_msg or "RESOURCE_EXHAUSTED" in error_msg:
                state['error'] = "API í• ë‹¹ëŸ‰ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
                state['is_finished'] = True
                return state

            # ê·¸ ì™¸ ì—ëŸ¬
            state['error'] = f"ë©´ì ‘ ì§„í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error_msg}"
            state['is_finished'] = True
            return state
    
    def new_question_generator(self, state: InterviewState) -> InterviewState:
        """ìƒˆë¡œìš´ ì£¼ì œ ì²« ì§ˆë¬¸ ìƒì„±"""
        try:
            logger.info(f"Generating first question for topic: {state.get('current_sub_topic')}")
            
            context_text = "\n\n".join(state.get('current_context', []))
            
            # ì²« ì§ˆë¬¸ í”„ë¡¬í”„íŠ¸
            prompt = f"""ë‹¹ì‹ ì€ ëŒ€í•™ ì…ì‹œ ë©´ì ‘ê´€ì…ë‹ˆë‹¤. ìƒˆë¡œìš´ ì£¼ì œì— ëŒ€í•œ ì²« ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”.

**ë©´ì ‘ ë‚œì´ë„**: {state['difficulty']}
**ìƒˆë¡œìš´ ì£¼ì œ**: {state.get('current_sub_topic')}

**ê´€ë ¨ í•™ìƒë¶€ ì •ë³´**:
{context_text}

**ì²« ì§ˆë¬¸ ìƒì„± ì§€ì¹¨**:
1. í•´ë‹¹ ì£¼ì œì™€ ê´€ë ¨ëœ ê°œë°©í˜• ì§ˆë¬¸ ìƒì„±
2. í•™ìƒì˜ ê²½í—˜ê³¼ ìƒê°ì„ ììœ ë¡­ê²Œ í‘œí˜„í•˜ê²Œ ìœ ë„
3. êµ¬ì²´ì ì¸ ì‚¬ë¡€ë¥¼ ìš”ì²­í•˜ëŠ” ë°©ì‹

ì£¼ì œ ê°€ì´ë“œë¼ì¸:
- ì¶œê²°: ì§€ê°/ê²°ì„ íŒ¨í„´ê³¼ ì‚¬ìœ , ì„±ì‹¤ì„±
- ì„±ì : ì „ê³µ ê³¼ëª© ì„±ì  ì¶”ì´ì™€ ë³€í™” ì´ìœ 
- ë™ì•„ë¦¬: í”„ë¡œì íŠ¸ ë‚´ ì—­í• ê³¼ ê¸°ìˆ ì  í•´ê²° ê³¼ì •
- ë¦¬ë”ì‹­: ê°ˆë“± ìƒí™©ì—ì„œì˜ í•´ê²° ë©”ì»¤ë‹ˆì¦˜
- ì¸ì„±/íƒœë„: í–‰íŠ¹ ê¸°ë¡ ê¸°ë°˜ ë³¸ì¸ì˜ ëŒ€í‘œ íŠ¹ì„±
- ì§„ë¡œ/ììœ¨: ì§€ì› ì „ê³µ ê´€ì‹¬ ê³„ê¸°ì™€ í™œë™ ì—°ê²°
- ë…ì„œ: ë„ì„œê°€ ê°€ì¹˜ê´€ ë° íƒêµ¬ì— ë¯¸ì¹œ ì˜í–¥
- ë´‰ì‚¬: í™œë™ì˜ ì§€ì†ì„±ê³¼ ë°°ìš´ ì 

ì²« ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”."""

            # JSON ìŠ¤í‚¤ë§ˆ
            schema = self.types.Schema(
                type=self.types.Type.OBJECT,
                properties={
                    "question": self.types.Schema(type=self.types.Type.STRING),
                    "context_summary": self.types.Schema(type=self.types.Type.STRING)
                },
                required=["question", "context_summary"]
            )
            
            response = self.client.models.generate_content(
                model=self.model,
                contents=prompt,
                config={
    "response_mime_type": "application/json",
    "response_json_schema": schema,
}
            )
            
            result = json.loads(response.text)

            # ìƒì„±ëœ ì§ˆë¬¸ì„ answer_logì— ì¶”ê°€
            from datetime import datetime
            log_entry = {
                "question": result['question'],
                "answer": "",
                "response_time": 0,
                "sub_topic": state.get('current_sub_topic', ''),
                "timestamp": datetime.now().isoformat(),
                "generated_question": result['question']
            }
            state['answer_log'] = state.get('answer_log', []) + [log_entry]

            return state

        except Exception as e:
            logger.error(f"Error generating new question: {e}")
            # ì—ëŸ¬ ë°œìƒ ì‹œ ë¹ˆ ì§ˆë¬¸ ë°˜í™˜
            return state
    
    def wrap_up(self, state: InterviewState) -> InterviewState:
        """ë©´ì ‘ ì¢…ë£Œ ë° ìš”ì•½ ìƒì„±"""
        db = None
        try:
            logger.info("Generating wrap-up summary")

            # ì „ì²´ ëŒ€í™” ê¸°ë¡ ë¶„ì„ (answer_log ì‚¬ìš©)
            answer_log = state.get('answer_log', [])

            # InterviewSession ì—…ë°ì´íŠ¸ (ì¢…ë£Œ ìƒíƒœ)
            session_id = state.get('session_id')
            if session_id:
                db = SessionLocal()
                try:
                    session = db.query(InterviewSession).filter(InterviewSession.id == session_id).first()
                    if session:
                        # í‰ê·  ì‘ë‹µ ì‹œê°„ ê³„ì‚°
                        avg_response_time = 0
                        if answer_log:
                            total_time = sum(log.get('response_time', 0) for log in answer_log)
                            avg_response_time = total_time // len(answer_log)

                        session.status = "COMPLETED"
                        session.avg_response_time = avg_response_time
                        session.completed_at = func.now()
                        db.commit()
                        logger.info(f"Updated interview session {session_id} to COMPLETED")
                finally:
                    db.close()

            # ê°„ë‹¨í•œ ì¢…ë£Œ ë©”ì‹œì§€ë§Œ ìƒì„± (ìƒì„¸ ë¶„ì„ì€ analyze_interview_resultì—ì„œ)
            closing_message = f"""ë©´ì ‘ì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤.

ğŸ“Š **ë©´ì ‘ ìš”ì•½**
- ì´ ì§ˆë¬¸ ìˆ˜: {len(answer_log)}ê°œ
- ì†Œìš” ì‹œê°„: {600 - state.get('remaining_time', 600)}ì´ˆ

ìƒì„¸ ë¶„ì„ ê²°ê³¼ëŠ” ë©´ì ‘ ì¢…ë£Œ í›„ í™•ì¸í•´ì£¼ì„¸ìš”."""

            state['interview_stage'] = "WRAP_UP"

            return state

        except Exception as e:
            logger.error(f"Error in wrap_up: {e}")
            state['interview_stage'] = "WRAP_UP"
            return state
    

    def initialize_interview(
        self,
        user_id: int,
        record_id: int,
        difficulty: str,
        first_answer: str,
        response_time: int,
        thread_id: str
    ) -> Dict[str, Any]:
        """
        ë©´ì ‘ ì´ˆê¸°í™” (ì²« ë‹µë³€ ì²˜ë¦¬)

        Args:
            user_id: ì‚¬ìš©ì ID
            record_id: ìƒê¸°ë¶€ ID
            difficulty: ë‚œì´ë„ (Easy, Normal, Hard)
            first_answer: ì²« ë‹µë³€ (ìê¸°ì†Œê°œ)
            response_time: ë‹µë³€ ì†Œìš” ì‹œê°„
            thread_id: LangGraph thread ID

        Returns:
            Dict with next_question, updated_state, is_finished
        """
        db = None
        try:
            logger.info(f"Initializing interview for record {record_id}, difficulty: {difficulty}")

            # InterviewSession ìƒì„±
            db = SessionLocal()
            interview_session = InterviewSession(
                user_id=user_id,
                record_id=record_id,
                thread_id=thread_id,
                difficulty=difficulty,
                status="IN_PROGRESS"
            )
            db.add(interview_session)
            db.commit()
            db.refresh(interview_session)
            logger.info(f"Created interview session: {interview_session.id}")

            # ì´ˆê¸° ìƒíƒœ ìƒì„± (ì²« ë²ˆì§¸ answer_log í•­ëª© ë¯¸ë¦¬ ì¶”ê°€)
            from datetime import datetime
            initial_answer_log = [{
                "question": "ìê¸°ì†Œê°œ ë¶€íƒë“œë¦½ë‹ˆë‹¤.",
                "answer": first_answer,
                "response_time": response_time,
                "sub_topic": "",
                "timestamp": datetime.now().isoformat()
            }]

            initial_state: InterviewState = {
                'difficulty': difficulty,
                'remaining_time': 600,  # 10ë¶„
                'interview_stage': 'INTRO',
                'current_context': [],
                'current_sub_topic': '',
                'asked_sub_topics': [],
                'answer_log': initial_answer_log,
                'next_action': '',
                'follow_up_count': 0,
                'session_id': interview_session.id,  # ì„¸ì…˜ ID ì €ì¥
                'record_id': record_id,
                'current_user_answer': first_answer,
                'current_response_time': response_time
            }

            # process_answer ì¬ì‚¬ìš©
            return self.process_answer(
                state=initial_state,
                user_answer=first_answer,
                response_time=response_time,
                thread_id=thread_id
            )

        except Exception as e:
            logger.error(f"Error initializing interview: {e}")
            if db:
                db.rollback()
            raise
        finally:
            if db:
                db.close()

    def get_state(self, thread_id: str) -> InterviewState:
        """
        thread_idë¡œ í˜„ì¬ ìƒíƒœ ì¡°íšŒ

        Args:
            thread_id: LangGraph thread ID

        Returns:
            í˜„ì¬ InterviewState
        """
        try:
            config = {"configurable": {"thread_id": thread_id}}

            # PostgresSaver ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ë‚´ì—ì„œ ìƒíƒœ ì¡°íšŒ
            with PostgresSaver.from_conn_string(self._conn_string) as checkpointer:
                # get_tupleë¡œ ì „ì²´ íŠœí”Œ ê°€ì ¸ì˜¤ê¸°
                result = checkpointer.get_tuple(config=config)

                if result is None:
                    raise ValueError(f"No state found for thread_id: {thread_id}")

                # result.checkpoint['channel_values']ì— ìš°ë¦¬ InterviewState ë°ì´í„°ê°€ ìˆìŒ
                return result.checkpoint['channel_values']

        except Exception as e:
            logger.error(f"Error getting state for thread_id {thread_id}: {e}")
            raise

    def process_answer(
        self,
        state: InterviewState,
        user_answer: str,
        response_time: int,
        thread_id: str
    ) -> str:
        """
        ë‹µë³€ ì²˜ë¦¬ ë° ë‹¤ìŒ ì§ˆë¬¸ ìƒì„± (LangGraph invoke ë°©ì‹)

        Args:
            state: í˜„ì¬ ë©´ì ‘ ìƒíƒœ (record_id í¬í•¨)
            user_answer: ì‚¬ìš©ì ë‹µë³€
            response_time: ë‹µë³€ ì†Œìš” ì‹œê°„
            thread_id: LangGraph thread ID (Checkpointerìš©)

        Returns:
            str: ë‹¤ìŒ ì§ˆë¬¸ í…ìŠ¤íŠ¸
        """
        try:
            # í˜„ì¬ ë‹µë³€ ì •ë³´ë§Œ stateì— ì„¤ì • (pass-through í•„ë“œ)
            state['current_user_answer'] = user_answer
            state['current_response_time'] = response_time

            # ë‚¨ì€ ì‹œê°„ ì²´í¬
            if state['remaining_time'] < 30:
                state['next_action'] = "wrap_up"

            # PostgresSaver ì»¨í…ìŠ¤íŠ¸ ë‚´ì—ì„œ ê·¸ë˜í”„ ì‹¤í–‰
            with PostgresSaver.from_conn_string(self._conn_string) as checkpointer:
                graph = self._build_workflow().compile(checkpointer=checkpointer)
                config = {"configurable": {"thread_id": thread_id}}
                result_state = graph.invoke(state, config=config)

                # answer_logì—ì„œ ë§ˆì§€ë§‰ ì§ˆë¬¸ ì¶”ì¶œ
                next_question = ""
                answer_log = result_state.get('answer_log', [])
                if answer_log:
                    # answer_logì— ì¶”ê°€ëœ ë§ˆì§€ë§‰ ì§ˆë¬¸ ì‚¬ìš©
                    # analyzerì—ì„œ ì´ë¯¸ ì´ì „ ì§ˆë¬¸ì„ ì €ì¥í–ˆìŒ
                    # ìƒˆë¡œ ìƒì„±ëœ ì§ˆë¬¸ì€ result_stateì˜ ë§ˆì§€ë§‰ í•­ëª©
                    for log in reversed(answer_log):
                        if 'generated_question' in log:
                            next_question = log['generated_question']
                            break

                    # ì—†ìœ¼ë©´ ë§ˆì§€ë§‰ question í•„ë“œ ì‚¬ìš©
                    if not next_question:
                        next_question = answer_log[-1].get('question', '')

                return next_question

        except Exception as e:
            error_msg = str(e)
            logger.error(f"Error processing answer: {e}", exc_info=True)

            # 429 í• ë‹¹ëŸ‰ ì´ˆê³¼ ì—ëŸ¬
            if "429" in error_msg or "RESOURCE_EXHAUSTED" in error_msg:
                raise Exception("API_QUOTA_EXCEEDED: Google Gemini API í• ë‹¹ëŸ‰ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")

            # ê·¸ ì™¸ ì—ëŸ¬
            raise Exception(f"ë©´ì ‘ ì§„í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error_msg}")

    def analyze_interview_result(self, thread_id: str) -> Dict[str, Any]:
        """
        ë©´ì ‘ ê²°ê³¼ ë¶„ì„ ë° ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±

        Args:
            thread_id: LangGraph thread ID

        Returns:
            ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸
        """
        db = None
        try:
            logger.info(f"Analyzing interview result for thread_id: {thread_id}")

            # 1. ìƒíƒœ ì¡°íšŒ
            state = self.get_state(thread_id)

            # 2. answer_logì—ì„œ ëŒ€í™” ìš”ì•½ ì¶”ì¶œ
            answer_log = state.get('answer_log', [])

            if not answer_log:
                return {
                    "error": "No interview data found",
                    "message": "ë©´ì ‘ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
                }

            # 3. í‰ê·  ì‘ë‹µ ì‹œê°„ ê³„ì‚°
            total_response_time = sum(log.get('response_time', 0) for log in answer_log)
            avg_response_time = total_response_time // len(answer_log) if answer_log else 0

            # 4. ì „ì²´ ì†Œìš” ì‹œê°„ ê³„ì‚°
            total_duration = 600 - state.get('remaining_time', 600)

            # 5. ëŒ€í™” ìš”ì•½ ìƒì„± (ì „ì²´ ë‹µë³€ ì‚¬ìš©)
            conversation_summary = []
            for log in answer_log:
                conversation_summary.append(f"Q: {log['question']}")
                conversation_summary.append(f"A: {log['answer']} (ì†Œìš”ì‹œê°„: {log['response_time']}ì´ˆ)")

            summary_text = "\n".join(conversation_summary)

            # 6. AI ë¶„ì„ í”„ë¡¬í”„íŠ¸
            prompt = f"""ë‹¹ì‹ ì€ ëŒ€í•™ ì…ì‹œ ë©´ì ‘ê´€ì…ë‹ˆë‹¤. ë©´ì ‘ ì¢…ë£Œ í›„ ì¢…í•© í‰ê°€ë¥¼ ìƒì„±í•˜ì„¸ìš”.

**ë©´ì ‘ ë‚œì´ë„**: {state['difficulty']}
**ì´ ë‹µë³€ ìˆ˜**: {len(answer_log)}
**í‰ê·  ì‘ë‹µ ì‹œê°„**: {avg_response_time}ì´ˆ

**ì „ì²´ ëŒ€í™” ë‚´ìš©**:
{summary_text}

**ì ìˆ˜ ì‚°ì • ê¸°ì¤€**:
- ì „ê³µì í•©ì„±: 0~25ì  (ì§€ì› ì „ê³µì— ëŒ€í•œ ì´í•´ë„, ê´€ë ¨ í™œë™ê³¼ì˜ ì—°ê²°ì„±)
- ì¸ì„±: 0~25ì  (íƒœë„, ì„±ì‹¤ì„±, íƒ€ì¸ì— ëŒ€í•œ ë°°ë ¤)
- ë°œì „ê°€ëŠ¥ì„±: 0~25ì  (í•™ìŠµ ì˜ì§€, ì„±ì¥ ë§ˆì¸ë“œ, ìê¸° ê°œì„  ë…¸ë ¥)
- ì˜ì‚¬ì†Œí†µëŠ¥ë ¥: 0~25ì  (ë…¼ë¦¬ì  ë§í•˜ê¸°, ëª…í™•í•œ í‘œí˜„, ê²½ì²­ íƒœë„)
- ì´ì : 0~100ì  (ìœ„ 4ê°œ ì˜ì—­ í•©ê³„)

**ê°•ì  íƒœê·¸ ì˜ˆì‹œ**: êµ¬ì²´ì  ì‚¬ë¡€ ì œì‹œ, ë…¼ë¦¬ì  êµ¬ì¡°ë¥¼ ê°€ì§, ìì‹ ê° ìˆëŠ” íƒœë„, êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ ì¸ìš©, ì„±ì‹¤í•œ ë‹µë³€ ë“±

**ë‹¨ì  íƒœê·¸ ì˜ˆì‹œ**: ë‹µë³€ ì‹œê°„ì´ ëŠë¦¼, ê·¼ê±° ë¶€ì¡±, ì§ˆë¬¸ ì˜ë„ ì¬í™•ì¸ í•„ìš”, ì¶”ìƒì ì¸ ë‹µë³€, ê²°ë¡ ì´ ë¶ˆëª…í™•í•¨ ë“±

**ìƒì„¸ ë¶„ì„ ê¸°ì¤€**:
- í‰ê°€: ì¢‹ìŒ/ë³´í†µ/ë‚˜ì¨ (ë‹µë³€ì˜ ì¶©ì‹¤ë„, êµ¬ì²´ì„±, ë…¼ë¦¬ì„± ê³ ë ¤)
- ê°œì„  í¬ì¸íŠ¸: "ë‚´ ì—­í• ì„ ë” ëª…í™•íˆ ê°•ì¡°í•˜ê¸°", "ê²°ë¡ ì„ ë¨¼ì € ë§í•˜ê³  êµ¬ì²´ ì‚¬ë¡€ ë§ë¶™ì´ê¸°" ë“±
- ë³´ì™„ í•„ìš”: "ë°°ìš´ ì ì„ ì „ê³µê³¼ ì—°ê²°í•˜ëŠ” ë¬¸ì¥ 1ì¤„ ì¶”ê°€", "êµ¬ì²´ì ì¸ ê²°ê³¼ ìˆ˜ì¹˜ ì–¸ê¸‰í•˜ê¸°" ë“±

**JSON í˜•ì‹ìœ¼ë¡œ ì¢…í•© í‰ê°€ë¥¼ ìƒì„±í•˜ì„¸ìš”.**

ê° ë‹µë³€ì— ëŒ€í•´ ì§ˆë¬¸ ë‚´ìš©, ë‹µë³€ ì‹œê°„, í‰ê°€, ê°œì„  í¬ì¸íŠ¸, ë³´ì™„ í•„ìš” í•­ëª©ì„ ë¶„ì„í•˜ì„¸ìš”."""

            # 7. JSON ìŠ¤í‚¤ë§ˆ
            schema = self.types.Schema(
                type=self.types.Type.OBJECT,
                properties={
                    "scores": self.types.Schema(
                        type=self.types.Type.OBJECT,
                        properties={
                            "ì „ê³µì í•©ì„±": self.types.Schema(type=self.types.Type.INTEGER, minimum=0, maximum=25),
                            "ì¸ì„±": self.types.Schema(type=self.types.Type.INTEGER, minimum=0, maximum=25),
                            "ë°œì „ê°€ëŠ¥ì„±": self.types.Schema(type=self.types.Type.INTEGER, minimum=0, maximum=25),
                            "ì˜ì‚¬ì†Œí†µëŠ¥ë ¥": self.types.Schema(type=self.types.Type.INTEGER, minimum=0, maximum=25),
                            "ì´ì ": self.types.Schema(type=self.types.Type.INTEGER, minimum=0, maximum=100)
                        },
                        required=["ì „ê³µì í•©ì„±", "ì¸ì„±", "ë°œì „ê°€ëŠ¥ì„±", "ì˜ì‚¬ì†Œí†µëŠ¥ë ¥", "ì´ì "]
                    ),
                    "strength_tags": self.types.Schema(
                        type=self.types.Type.ARRAY,
                        items=self.types.Schema(type=self.types.Type.STRING)
                    ),
                    "weakness_tags": self.types.Schema(
                        type=self.types.Type.ARRAY,
                        items=self.types.Schema(type=self.types.Type.STRING)
                    ),
                    "detailed_analysis": self.types.Schema(
                        type=self.types.Type.ARRAY,
                        items=self.types.Schema(
                            type=self.types.Type.OBJECT,
                            properties={
                                "question": self.types.Schema(type=self.types.Type.STRING, description="ì§ˆë¬¸ ë‚´ìš©"),
                                "response_time": self.types.Schema(type=self.types.Type.INTEGER, description="ë‹µë³€ ì‹œê°„(ì´ˆ)"),
                                "evaluation": self.types.Schema(type=self.types.Type.STRING, description="í‰ê°€ (ì¢‹ìŒ/ë³´í†µ/ë‚˜ì¨)"),
                                "improvement_point": self.types.Schema(type=self.types.Type.STRING, description="ê°œì„  í¬ì¸íŠ¸"),
                                "supplement_needed": self.types.Schema(type=self.types.Type.STRING, description="ë³´ì™„ í•„ìš” ì‚¬í•­")
                            },
                            required=["question", "response_time", "evaluation", "improvement_point", "supplement_needed"]
                        )
                    )
                },
                required=["scores", "strength_tags", "weakness_tags", "detailed_analysis"]
            )

            # 8. Gemini í˜¸ì¶œ
            response = self.client.models.generate_content(
                model=self.model,
                contents=prompt,
                config={
                    "response_mime_type": "application/json",
                    "response_json_schema": schema,
                }
            )

            result = json.loads(response.text)

            # 9. InterviewSession ì—…ë°ì´íŠ¸
            db = SessionLocal()
            interview_session = db.query(InterviewSession).filter(
                InterviewSession.thread_id == thread_id
            ).first()

            if interview_session:
                interview_session.status = "COMPLETED"
                interview_session.completed_at = func.now()
                interview_session.avg_response_time = avg_response_time
                interview_session.total_questions = len(answer_log)
                interview_session.total_duration = total_duration
                interview_session.final_report = result
                db.commit()
                logger.info(f"Updated interview session {interview_session.id} to COMPLETED")

            # 10. ê²°ê³¼ ë°˜í™˜
            return {
                "scores": result.get("scores", {}),
                "strength_tags": result.get("strength_tags", []),
                "weakness_tags": result.get("weakness_tags", []),
                "detailed_analysis": result.get("detailed_analysis", [])
            }

        except Exception as e:
            logger.error(f"Error analyzing interview result: {e}")
            if db:
                db.rollback()
            return {
                "error": str(e),
                "message": "ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            }
        finally:
            if db:
                db.close()


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
interview_graph = InterviewGraph()
