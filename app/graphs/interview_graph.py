"""ì‹¤ì‹œê°„ ë©´ì ‘ LangGraph êµ¬í˜„

ê¼¬ë¦¬ ì§ˆë¬¸(Tail Questions) ì‹œìŠ¤í…œì„ í†µí•´ ì‹¬ì¸µì ì¸ ë©´ì ‘ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
ìƒíƒœ ì €ì¥ì€ LangGraphì˜ AsyncPostgresSaver Checkpointerê°€ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
"""
from typing import TypedDict, List, Dict, Any, Optional, Annotated
from operator import add
from langgraph.graph import StateGraph, END
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langgraph.checkpoint.postgres.aio import AsyncPostgresSaver
from pydantic import BaseModel, Field
from google import genai
from google.genai import types
from config import settings
import logging
import json
import asyncpg

logger = logging.getLogger(__name__)


# ==================== State ì •ì˜ ====================

class InterviewState(TypedDict):
    """ë©´ì ‘ ìƒíƒœ"""

    # ê¸°ë³¸ ì„¤ì •
    difficulty: str                    # ë©´ì ‘ ë‚œì´ë„ (Easy, Normal, Hard)
    remaining_time: int                # ë‚¨ì€ ì‹œê°„ (ì´ˆ ë‹¨ìœ„)
    interview_stage: str               # [INTRO, MAIN, WRAP_UP]

    # ëŒ€í™” ì»¨í…ìŠ¤íŠ¸
    conversation_history: List[BaseMessage]  # ëŒ€í™” ê¸°ë¡
    current_context: List[str]         # í˜„ì¬ ì§ˆë¬¸/ì£¼ì œì™€ ê´€ë ¨ëœ í•™ìƒë¶€ ì²­í¬ ë¦¬ìŠ¤íŠ¸
    current_sub_topic: str             # í˜„ì¬ ì§„í–‰ ì¤‘ì¸ ì„¸ë¶€ ì£¼ì œ
    asked_sub_topics: List[str]        # ì´ë¯¸ ì™„ë£Œëœ ì„¸ë¶€ ì£¼ì œ ë¦¬ìŠ¤íŠ¸

    # ë‹µë³€ ê¸°ë¡ (ë‚˜ì¤‘ì— ë¶„ì„ APIì—ì„œ ì‚¬ìš©)
    answer_log: List[Dict]             # [{question, answer, response_time, timestamp}]

    # ë‚´ë¶€ ìƒíƒœ
    next_action: str                   # [follow_up, new_topic, wrap_up]
    follow_up_count: int               # í˜„ì¬ ì£¼ì œì— ëŒ€í•œ ê¼¬ë¦¬ ì§ˆë¬¸ íšŸìˆ˜

    # í˜„ì¬ ë‹µë³€ ì •ë³´ (graph ì‹¤í–‰ ì‹œ í•„ìš”)
    current_user_answer: str           # í˜„ì¬ ì‚¬ìš©ì ë‹µë³€
    current_response_time: int         # í˜„ì¬ ë‹µë³€ ì†Œìš” ì‹œê°„
    record_id: int                     # ìƒê¸°ë¶€ ID


# ==================== Pydantic ëª¨ë¸ ====================

class AnalyzerDecision(BaseModel):
    """ë¶„ì„ê¸° ê²°ì • ëª¨ë¸ - ê¼¬ë¦¬ì§ˆë¬¸ ì—¬ë¶€ë§Œ íŒë‹¨"""
    action: str = Field(description="ë‹¤ìŒ ì•¡ì…˜ (follow_up, new_topic, wrap_up)")
    reasoning: str = Field(description="ê²°ì • ê·¼ê±°")


class GeneratedQuestion(BaseModel):
    """ìƒì„±ëœ ì§ˆë¬¸ ëª¨ë¸"""
    question: str = Field(description="ì§ˆë¬¸ ë‚´ìš©")
    context_summary: str = Field(description="ì‚¬ìš©ëœ ì»¨í…ìŠ¤íŠ¸ ìš”ì•½")


# ==================== í•˜ìœ„ ì£¼ì œ ì •ì˜ ====================

SUB_TOPICS = [
    "ì¶œê²°", "ì„±ì ", "ë™ì•„ë¦¬", "ë¦¬ë”ì‹­", 
    "ì¸ì„±/íƒœë„", "ì§„ë¡œ/ììœ¨", "ë…ì„œ", "ë´‰ì‚¬"
]


# ==================== Interview Graph ====================

class InterviewGraph:
    """ì‹¤ì‹œê°„ ë©´ì ‘ LangGraph"""

    def __init__(self):
        # Google GenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.client = genai.Client(api_key=settings.google_api_key)
        self.model = "gemini-2.5-flash-lite"
        self.types = types

        # Postgres Checkpointer ì´ˆê¸°í™” (async lazy)
        self.checkpointer = None
        self._pool = None
        self._graph = None

    async def _init_checkpointer(self):
        """Checkpointer ì´ˆê¸°í™” (AsyncPostgresSaver ì‚¬ìš©)"""
        try:
            if self.checkpointer is not None:
                return self.checkpointer

            # database_urlì„ asyncpgìš©ìœ¼ë¡œ ë³€í™˜ (postgresql+psycopg2 â†’ postgresql)
            conn_string = settings.database_url
            if conn_string.startswith("postgresql+psycopg://"):
                conn_string = conn_string.replace("postgresql+psycopg://", "postgresql://", 1)
            elif conn_string.startswith("postgresql+psycopg2://"):
                conn_string = conn_string.replace("postgresql+psycopg2://", "postgresql://", 1)

            # asyncpg Pool ìƒì„±
            self._pool = await asyncpg.create_pool(conn_string, min_size=1, max_size=10)

            # AsyncPostgresSaver ìƒì„±
            self.checkpointer = AsyncPostgresSaver(self._pool)

            logger.info("AsyncPostgresSaver checkpointer initialized successfully")
            return self.checkpointer

        except Exception as e:
            logger.error(f"Failed to initialize checkpointer: {e}")
            return None

    async def get_graph(self):
        """ê·¸ë˜í”„ ë°˜í™˜ (lazy initialization with checkpointer)"""
        if self._graph is None:
            # Checkpointer ì´ˆê¸°í™” ì‹œë„
            if self.checkpointer is None:
                await self._init_checkpointer()

            # ê·¸ë˜í”„ ë¹Œë“œ
            self._graph = self._build_graph()

        return self._graph
    
    def _build_graph(self) -> StateGraph:
        """LangGraph ë¹Œë“œ"""
        workflow = StateGraph(InterviewState)

        # ë…¸ë“œ ì¶”ê°€
        workflow.add_node("analyzer", self.analyzer)
        workflow.add_node("retrieve_new_topic", self.retrieve_new_topic)
        workflow.add_node("follow_up_generator", self.follow_up_generator)
        workflow.add_node("new_question_generator", self.new_question_generator)
        workflow.add_node("wrap_up", self.wrap_up)

        # ì—”íŠ¸ë¦¬ í¬ì¸íŠ¸
        workflow.set_entry_point("analyzer")

        # ì¡°ê±´ë¶€ ì—£ì§€
        workflow.add_conditional_edges(
            "analyzer",
            self.decide_next_action,
            {
                "follow_up": "follow_up_generator",
                "new_topic": "retrieve_new_topic",
                "wrap_up": "wrap_up"
            }
        )

        # ì¼ë°˜ ì—£ì§€
        workflow.add_edge("retrieve_new_topic", "new_question_generator")
        workflow.add_edge("follow_up_generator", END)
        workflow.add_edge("new_question_generator", END)
        workflow.add_edge("wrap_up", END)

        # Checkpointerì™€ í•¨ê»˜ ì»´íŒŒì¼
        return workflow.compile(checkpointer=self.checkpointer)
    
    async def analyzer(self, state: InterviewState) -> InterviewState:
        """ë‹µë³€ ë¶„ì„ ë° ë‹¤ìŒ ì•¡ì…˜ ê²°ì • (ê¼¬ë¦¬ì§ˆë¬¸ ì—¬ë¶€ë§Œ íŒë‹¨)"""
        try:
            logger.info(f"Analyzing answer for topic: {state.get('current_sub_topic', 'INTRO')}")

            # í˜„ì¬ ë‹µë³€ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            user_answer = state.get('current_user_answer', '')
            response_time = state.get('current_response_time', 0)

            # ë§ˆì§€ë§‰ ì§ˆë¬¸ ê°€ì ¸ì˜¤ê¸°
            last_question = ""
            if state['conversation_history']:
                for msg in reversed(state['conversation_history']):
                    if isinstance(msg, AIMessage):
                        last_question = msg.content
                        break

            # ì»¨í…ìŠ¤íŠ¸ í…ìŠ¤íŠ¸ êµ¬ì„±
            context_text = "\\n\\n".join(state.get('current_context', []))

            # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            prompt = f"""ë‹¹ì‹ ì€ ëŒ€í•™ ì…ì‹œ ë©´ì ‘ê´€ì…ë‹ˆë‹¤. í•™ìƒì˜ ë‹µë³€ì„ ë¶„ì„í•˜ê³  ë‹¤ìŒ ë‹¨ê³„ë¥¼ ê²°ì •í•˜ì„¸ìš”.

**ë©´ì ‘ ë‚œì´ë„**: {state['difficulty']}
**í˜„ì¬ ì£¼ì œ**: {state.get('current_sub_topic', 'ìê¸°ì†Œê°œ')}
**ë‚¨ì€ ì‹œê°„**: {state['remaining_time']}ì´ˆ

**ì´ì „ ì§ˆë¬¸**:
{last_question}

**í•™ìƒ ë‹µë³€** (ì†Œìš” ì‹œê°„: {response_time}ì´ˆ):
{user_answer}

**ê´€ë ¨ í•™ìƒë¶€ ì •ë³´**:
{context_text if context_text else "í•´ë‹¹ ì—†ìŒ"}

**ë¶„ì„ ì§€ì¹¨**:
ë‹¤ìŒ ì•¡ì…˜ì„ ê²°ì •í•˜ì„¸ìš”:
   - follow_up: ë‹µë³€ì´ ë¶ˆì¶©ë¶„í•˜ê±°ë‚˜ ë” ê¹Šì€ íŒŒê¸°ê°€ í•„ìš”í•  ë•Œ (êµ¬ì²´ì  ì‚¬ë¡€ ë¶€ì¡±, ë…¼ë¦¬ì  í—ˆì , íŒë‹¨ ê·¼ê±° ë¶ˆëª…í™• ë“±)
   - new_topic: ë‹µë³€ì´ ì¶©ì‹¤í•˜ê³  êµ¬ì²´ì ì´ì–´ì„œ ì£¼ì œë¥¼ ë°”ê¿€ ë•Œ
   - wrap_up: ì‹œê°„ì´ ë¶€ì¡±í•˜ê±°ë‚˜(30ì´ˆ ë¯¸ë§Œ) ë” ì´ìƒ ì§ˆë¬¸í•  ì£¼ì œê°€ ì—†ì„ ë•Œ

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”."""

            # JSON ìŠ¤í‚¤ë§ˆ (ê°„ì†Œí™”)
            schema = self.types.Schema(
                type=self.types.Type.OBJECT,
                properties={
                    "action": self.types.Schema(type=self.types.Type.STRING, description="ë‹¤ìŒ ì•¡ì…˜ (follow_up, new_topic, wrap_up)"),
                    "reasoning": self.types.Schema(type=self.types.Type.STRING, description="ê²°ì • ê·¼ê±°")
                },
                required=["action", "reasoning"]
            )
            
            # Gemini í˜¸ì¶œ
            response = self.client.models.generate_content(
                model=self.model,
                contents=prompt,
                config={
                    "response_mime_type": "application/json",
                    "response_json_schema": schema,
                }
            )
            
            result = json.loads(response.text)
            
            # ë‹µë³€ ë¡œê·¸ ì €ì¥
            from datetime import datetime
            log_entry = {
                "question": last_question,
                "answer": user_answer,
                "response_time": response_time,
                "sub_topic": state.get('current_sub_topic', ''),
                "timestamp": datetime.now().isoformat()
            }
            
            state['answer_log'].append(log_entry)
            
            # ë‹¤ìŒ ì•¡ì…˜ ì €ì¥
            state['next_action'] = result['action']
            
            logger.info(f"Analysis complete: {result['action']} - {result['reasoning']}")
            return state
            
        except Exception as e:
            logger.error(f"Error in analyzer: {e}")
            state['next_action'] = "wrap_up"
            return state
    
    def decide_next_action(self, state: InterviewState) -> str:
        """ë‹¤ìŒ ì•¡ì…˜ ê²°ì • (Conditional Edge)"""
        return state.get('next_action', 'wrap_up')
    
    async def retrieve_new_topic(
        self, 
        state: InterviewState
    ) -> InterviewState:
        """ìƒˆë¡œìš´ ì£¼ì œ ê²€ìƒ‰"""
        try:
            # ë¯¸ì¤‘ë³µ ì£¼ì œ ì„ íƒ
            remaining_topics = [
                topic for topic in SUB_TOPICS 
                if topic not in state.get('asked_sub_topics', [])
            ]
            
            if not remaining_topics:
                logger.info("No more topics available")
                state['next_action'] = "wrap_up"
                return state
            
            # ëœë¤ ì„ íƒ (ë˜ëŠ” ì „ëµì  ì„ íƒ)
            import random
            new_topic = random.choice(remaining_topics)
            
            logger.info(f"Selected new topic: {new_topic}")
            
            # ë²¡í„° DBì—ì„œ ê´€ë ¨ ì²­í¬ ê²€ìƒ‰
            from app.services.vector_service import vector_service
            
            chunks = await vector_service.search_chunks_by_topic(
                record_id=state['record_id'],
                topic=new_topic
            )
            
            state['current_sub_topic'] = new_topic
            state['current_context'] = chunks  # ì´ë¯¸ text ë¦¬ìŠ¤íŠ¸
            state['asked_sub_topics'].append(new_topic)
            state['follow_up_count'] = 0
            
            return state
            
        except Exception as e:
            logger.error(f"Error retrieving new topic: {e}")
            state['next_action'] = "wrap_up"
            return state
    
    async def follow_up_generator(self, state: InterviewState) -> InterviewState:
        """ê¼¬ë¦¬ ì§ˆë¬¸ ìƒì„±"""
        try:
            logger.info(f"Generating follow-up question for: {state.get('current_sub_topic')}")
            
            # ë§ˆì§€ë§‰ ë‹µë³€ ê°€ì ¸ì˜¤ê¸°
            last_answer = ""
            if state['conversation_history']:
                for msg in reversed(state['conversation_history']):
                    if isinstance(msg, HumanMessage):
                        last_answer = msg.content
                        break
            
            context_text = "\n\n".join(state.get('current_context', []))
            
            # ê¼¬ë¦¬ ì§ˆë¬¸ í”„ë¡¬í”„íŠ¸
            prompt = f"""ë‹¹ì‹ ì€ ëŒ€í•™ ì…ì‹œ ë©´ì ‘ê´€ì…ë‹ˆë‹¤. í•™ìƒì˜ ë‹µë³€ì— ëŒ€í•´ ê¼¬ë¦¬ ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”.

**ë©´ì ‘ ë‚œì´ë„**: {state['difficulty']}
**í˜„ì¬ ì£¼ì œ**: {state.get('current_sub_topic')}
**ê¼¬ë¦¬ ì§ˆë¬¸ íšŸìˆ˜**: {state.get('follow_up_count', 0) + 1}íšŒì°¨

**ì´ì „ ë‹µë³€**:
{last_answer}

**ê´€ë ¨ í•™ìƒë¶€ ì •ë³´**:
{context_text}

**ê¼¬ë¦¬ ì§ˆë¬¸ ìƒì„± ì§€ì¹¨**:
1. ë‹µë³€ì—ì„œ ì–¸ê¸‰ëœ êµ¬ì²´ì  ì‚¬ë¡€, íŒë‹¨ ê·¼ê±°, ë°°ìš´ ì ì„ ì§‘ìš”í•˜ê²Œ ìºë¬»ìœ¼ì„¸ìš”.
2. "ì™œ ê·¸ë ‡ê²Œ ìƒê°í–ˆë‚˜?", "êµ¬ì²´ì ìœ¼ë¡œ ì–´ë–¤ ê²°ê³¼ì˜€ë‚˜?", "ê·¸ ê³¼ì •ì—ì„œ ì–´ë–¤ ê³ ë¯¼ì´ ìˆì—ˆë‚˜?" ë“±ì˜ íŒ¨í„´ í™œìš©
3. Hard ëª¨ë“œì—ì„œëŠ” ë…¼ë¦¬ì  í—ˆì ì„ ì°Œë¥´ëŠ” ì••ë°• ì§ˆë¬¸ ìƒì„±
4. í•™ìƒë¶€ ì •ë³´ì™€ êµì°¨ ê²€ì¦í•˜ì—¬ ì§ˆë¬¸

ë‹¤ìŒ ê¼¬ë¦¬ ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”."""

            # JSON ìŠ¤í‚¤ë§ˆ
            schema = self.types.Schema(
                type=self.types.Type.OBJECT,
                properties={
                    "question": self.types.Schema(type=self.types.Type.STRING),
                    "context_summary": self.types.Schema(type=self.types.Type.STRING)
                },
                required=["question", "context_summary"]
            )
            
            response = self.client.models.generate_content(
                model=self.model,
                contents=prompt,
                config=self.types.GenerateContentConfig(
                    response_mime_type="application/json",
                    response_schema=schema,
                    temperature=0.8
                )
            )
            
            result = json.loads(response.text)
            
            # ì§ˆë¬¸ì„ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
            state['conversation_history'].append(AIMessage(content=result['question']))
            state['follow_up_count'] = state.get('follow_up_count', 0) + 1
            
            return state
            
        except Exception as e:
            logger.error(f"Error generating follow-up question: {e}")
            state['conversation_history'].append(
                AIMessage(content="ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
            )
            return state
    
    async def new_question_generator(self, state: InterviewState) -> InterviewState:
        """ìƒˆë¡œìš´ ì£¼ì œ ì²« ì§ˆë¬¸ ìƒì„±"""
        try:
            logger.info(f"Generating first question for topic: {state.get('current_sub_topic')}")
            
            context_text = "\n\n".join(state.get('current_context', []))
            
            # ì²« ì§ˆë¬¸ í”„ë¡¬í”„íŠ¸
            prompt = f"""ë‹¹ì‹ ì€ ëŒ€í•™ ì…ì‹œ ë©´ì ‘ê´€ì…ë‹ˆë‹¤. ìƒˆë¡œìš´ ì£¼ì œì— ëŒ€í•œ ì²« ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”.

**ë©´ì ‘ ë‚œì´ë„**: {state['difficulty']}
**ìƒˆë¡œìš´ ì£¼ì œ**: {state.get('current_sub_topic')}

**ê´€ë ¨ í•™ìƒë¶€ ì •ë³´**:
{context_text}

**ì²« ì§ˆë¬¸ ìƒì„± ì§€ì¹¨**:
1. í•´ë‹¹ ì£¼ì œì™€ ê´€ë ¨ëœ ê°œë°©í˜• ì§ˆë¬¸ ìƒì„±
2. í•™ìƒì˜ ê²½í—˜ê³¼ ìƒê°ì„ ììœ ë¡­ê²Œ í‘œí˜„í•˜ê²Œ ìœ ë„
3. êµ¬ì²´ì ì¸ ì‚¬ë¡€ë¥¼ ìš”ì²­í•˜ëŠ” ë°©ì‹

ì£¼ì œ ê°€ì´ë“œë¼ì¸:
- ì¶œê²°: ì§€ê°/ê²°ì„ íŒ¨í„´ê³¼ ì‚¬ìœ , ì„±ì‹¤ì„±
- ì„±ì : ì „ê³µ ê³¼ëª© ì„±ì  ì¶”ì´ì™€ ë³€í™” ì´ìœ 
- ë™ì•„ë¦¬: í”„ë¡œì íŠ¸ ë‚´ ì—­í• ê³¼ ê¸°ìˆ ì  í•´ê²° ê³¼ì •
- ë¦¬ë”ì‹­: ê°ˆë“± ìƒí™©ì—ì„œì˜ í•´ê²° ë©”ì»¤ë‹ˆì¦˜
- ì¸ì„±/íƒœë„: í–‰íŠ¹ ê¸°ë¡ ê¸°ë°˜ ë³¸ì¸ì˜ ëŒ€í‘œ íŠ¹ì„±
- ì§„ë¡œ/ììœ¨: ì§€ì› ì „ê³µ ê´€ì‹¬ ê³„ê¸°ì™€ í™œë™ ì—°ê²°
- ë…ì„œ: ë„ì„œê°€ ê°€ì¹˜ê´€ ë° íƒêµ¬ì— ë¯¸ì¹œ ì˜í–¥
- ë´‰ì‚¬: í™œë™ì˜ ì§€ì†ì„±ê³¼ ë°°ìš´ ì 

ì²« ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”."""

            # JSON ìŠ¤í‚¤ë§ˆ
            schema = self.types.Schema(
                type=self.types.Type.OBJECT,
                properties={
                    "question": self.types.Schema(type=self.types.Type.STRING),
                    "context_summary": self.types.Schema(type=self.types.Type.STRING)
                },
                required=["question", "context_summary"]
            )
            
            response = self.client.models.generate_content(
                model=self.model,
                contents=prompt,
                config={
    "response_mime_type": "application/json",
    "response_json_schema": schema,
}
            )
            
            result = json.loads(response.text)
            
            # ì§ˆë¬¸ì„ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
            state['conversation_history'].append(AIMessage(content=result['question']))
            
            return state
            
        except Exception as e:
            logger.error(f"Error generating new question: {e}")
            state['conversation_history'].append(
                AIMessage(content=f"{state.get('current_sub_topic')} ê´€ë ¨ ê²½í—˜ì„ ë§ì”€í•´ ì£¼ì‹œê² ìŠµë‹ˆê¹Œ?")
            )
            return state
    
    async def wrap_up(self, state: InterviewState) -> InterviewState:
        """ë©´ì ‘ ì¢…ë£Œ ë° ìš”ì•½ ìƒì„±"""
        try:
            logger.info("Generating wrap-up summary")

            # ì „ì²´ ëŒ€í™” ê¸°ë¡ ë¶„ì„ (answer_log ì‚¬ìš©)
            answer_log = state.get('answer_log', [])

            # ê°„ë‹¨í•œ ì¢…ë£Œ ë©”ì‹œì§€ë§Œ ìƒì„± (ìƒì„¸ ë¶„ì„ì€ analyze_interview_resultì—ì„œ)
            closing_message = f"""ë©´ì ‘ì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤.

ğŸ“Š **ë©´ì ‘ ìš”ì•½**
- ì´ ì§ˆë¬¸ ìˆ˜: {len(answer_log)}ê°œ
- ì†Œìš” ì‹œê°„: {600 - state.get('remaining_time', 600)}ì´ˆ

ìƒì„¸ ë¶„ì„ ê²°ê³¼ëŠ” ë©´ì ‘ ì¢…ë£Œ í›„ í™•ì¸í•´ì£¼ì„¸ìš”."""

            state['conversation_history'].append(AIMessage(content=closing_message))
            state['interview_stage'] = "WRAP_UP"

            return state

        except Exception as e:
            logger.error(f"Error in wrap_up: {e}")
            state['conversation_history'].append(
                AIMessage(content="ë©´ì ‘ì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤.")
            )
            state['interview_stage'] = "WRAP_UP"
            return state
    

    async def initialize_interview(
        self,
        record_id: int,
        difficulty: str,
        first_answer: str,
        response_time: int,
        thread_id: str
    ) -> Dict[str, Any]:
        """
        ë©´ì ‘ ì´ˆê¸°í™” (ì²« ë‹µë³€ ì²˜ë¦¬)

        Args:
            record_id: ìƒê¸°ë¶€ ID
            difficulty: ë‚œì´ë„ (Easy, Normal, Hard)
            first_answer: ì²« ë‹µë³€ (ìê¸°ì†Œê°œ)
            response_time: ë‹µë³€ ì†Œìš” ì‹œê°„
            thread_id: LangGraph thread ID

        Returns:
            Dict with next_question, updated_state, is_finished
        """
        try:
            logger.info(f"Initializing interview for record {record_id}, difficulty: {difficulty}")

            # ì´ˆê¸° ìƒíƒœ ìƒì„±
            initial_state: InterviewState = {
                'difficulty': difficulty,
                'remaining_time': 600,  # 10ë¶„
                'interview_stage': 'INTRO',
                'conversation_history': [
                    AIMessage(content="ìê¸°ì†Œê°œ ë¶€íƒë“œë¦½ë‹ˆë‹¤.")
                ],
                'current_context': [],
                'current_sub_topic': '',
                'asked_sub_topics': [],
                'answer_log': [],
                'next_action': '',
                'follow_up_count': 0,
                'current_user_answer': first_answer,
                'current_response_time': response_time,
                'record_id': record_id
            }

            # process_answer ì¬ì‚¬ìš©
            return await self.process_answer(
                state=initial_state,
                user_answer=first_answer,
                response_time=response_time,
                thread_id=thread_id
            )

        except Exception as e:
            logger.error(f"Error initializing interview: {e}")
            raise

    async def get_state(self, thread_id: str) -> InterviewState:
        """
        thread_idë¡œ í˜„ì¬ ìƒíƒœ ì¡°íšŒ

        Args:
            thread_id: LangGraph thread ID

        Returns:
            í˜„ì¬ InterviewState
        """
        try:
            # Checkpointer ì´ˆê¸°í™” í™•ì¸
            if self.checkpointer is None:
                await self._init_checkpointer()

            if self.checkpointer is None:
                raise ValueError("Checkpointer is not available")

            config = {"configurable": {"thread_id": thread_id}}

            # Checkpointerì—ì„œ ìƒíƒœ ì¡°íšŒ
            state_snapshot = await self.checkpointer.aget(config=config)

            if state_snapshot is None:
                raise ValueError(f"No state found for thread_id: {thread_id}")

            return state_snapshot.values

        except Exception as e:
            logger.error(f"Error getting state for thread_id {thread_id}: {e}")
            raise

    async def process_answer(
        self,
        state: InterviewState,
        user_answer: str,
        response_time: int,
        thread_id: str
    ) -> str:
        """
        ë‹µë³€ ì²˜ë¦¬ ë° ë‹¤ìŒ ì§ˆë¬¸ ìƒì„± (LangGraph invoke ë°©ì‹)

        Args:
            state: í˜„ì¬ ë©´ì ‘ ìƒíƒœ (record_id í¬í•¨)
            user_answer: ì‚¬ìš©ì ë‹µë³€
            response_time: ë‹µë³€ ì†Œìš” ì‹œê°„
            thread_id: LangGraph thread ID (Checkpointerìš©)

        Returns:
            str: ë‹¤ìŒ ì§ˆë¬¸ í…ìŠ¤íŠ¸
        """
        try:
            # ì‚¬ìš©ì ë‹µë³€ì„ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
            state['conversation_history'].append(HumanMessage(content=user_answer))

            # ì‹œê°„ ì—…ë°ì´íŠ¸
            state['remaining_time'] -= response_time

            # í˜„ì¬ ë‹µë³€ ì •ë³´ë¥¼ stateì— ì„¤ì •
            state['current_user_answer'] = user_answer
            state['current_response_time'] = response_time

            # ë‚¨ì€ ì‹œê°„ ì²´í¬
            if state['remaining_time'] < 30:
                state['next_action'] = "wrap_up"

            # LangGraph invoke
            graph = await self.get_graph()
            config = {"configurable": {"thread_id": thread_id}}
            result_state = await graph.ainvoke(state, config=config)
            
            # ë§ˆì§€ë§‰ AI ë©”ì‹œì§€(ì§ˆë¬¸) ì¶”ì¶œ
            next_question = ""
            if result_state['conversation_history']:
                for msg in reversed(result_state['conversation_history']):
                    if isinstance(msg, AIMessage):
                        next_question = msg.content
                        break

            return next_question

        except Exception as e:
            logger.error(f"Error processing answer: {e}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë©´ì ‘ì„ ì¢…ë£Œí•©ë‹ˆë‹¤."

    async def analyze_interview_result(self, thread_id: str) -> Dict[str, Any]:
        """
        ë©´ì ‘ ê²°ê³¼ ë¶„ì„ ë° ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±

        Args:
            thread_id: LangGraph thread ID

        Returns:
            ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸
        """
        try:
            logger.info(f"Analyzing interview result for thread_id: {thread_id}")

            # 1. ìƒíƒœ ì¡°íšŒ
            state = await self.get_state(thread_id)

            # 2. answer_logì—ì„œ ëŒ€í™” ìš”ì•½ ì¶”ì¶œ
            answer_log = state.get('answer_log', [])

            if not answer_log:
                return {
                    "error": "No interview data found",
                    "message": "ë©´ì ‘ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
                }

            # 3. ëŒ€í™” ìš”ì•½ ìƒì„± (ì „ì²´ ë‹µë³€ ì‚¬ìš©)
            conversation_summary = []
            for log in answer_log:
                conversation_summary.append(f"Q: {log['question']}")
                conversation_summary.append(f"A: {log['answer']} (ì†Œìš”ì‹œê°„: {log['response_time']}ì´ˆ)")

            summary_text = "\n".join(conversation_summary)

            # 6. AI ë¶„ì„ í”„ë¡¬í”„íŠ¸
            prompt = f"""ë‹¹ì‹ ì€ ëŒ€í•™ ì…ì‹œ ë©´ì ‘ê´€ì…ë‹ˆë‹¤. ë©´ì ‘ ì¢…ë£Œ í›„ ì¢…í•© í‰ê°€ë¥¼ ìƒì„±í•˜ì„¸ìš”.

**ë©´ì ‘ ë‚œì´ë„**: {state['difficulty']}
**ì´ ë‹µë³€ ìˆ˜**: {len(answer_log)}
**í‰ê·  ì‘ë‹µ ì‹œê°„**: {avg_response_time}ì´ˆ

**ì „ì²´ ëŒ€í™” ë‚´ìš©**:
{summary_text}

**ì ìˆ˜ ì‚°ì • ê¸°ì¤€**:
- ì „ê³µì í•©ì„±: 0~25ì  (ì§€ì› ì „ê³µì— ëŒ€í•œ ì´í•´ë„, ê´€ë ¨ í™œë™ê³¼ì˜ ì—°ê²°ì„±)
- ì¸ì„±: 0~25ì  (íƒœë„, ì„±ì‹¤ì„±, íƒ€ì¸ì— ëŒ€í•œ ë°°ë ¤)
- ë°œì „ê°€ëŠ¥ì„±: 0~25ì  (í•™ìŠµ ì˜ì§€, ì„±ì¥ ë§ˆì¸ë“œ, ìê¸° ê°œì„  ë…¸ë ¥)
- ì˜ì‚¬ì†Œí†µëŠ¥ë ¥: 0~25ì  (ë…¼ë¦¬ì  ë§í•˜ê¸°, ëª…í™•í•œ í‘œí˜„, ê²½ì²­ íƒœë„)
- ì´ì : 0~100ì  (ìœ„ 4ê°œ ì˜ì—­ í•©ê³„)

**ê°•ì  íƒœê·¸ ì˜ˆì‹œ**: êµ¬ì²´ì  ì‚¬ë¡€ ì œì‹œ, ë…¼ë¦¬ì  êµ¬ì¡°ë¥¼ ê°€ì§, ìì‹ ê° ìˆëŠ” íƒœë„, êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ ì¸ìš©, ì„±ì‹¤í•œ ë‹µë³€ ë“±

**ë‹¨ì  íƒœê·¸ ì˜ˆì‹œ**: ë‹µë³€ ì‹œê°„ì´ ëŠë¦¼, ê·¼ê±° ë¶€ì¡±, ì§ˆë¬¸ ì˜ë„ ì¬í™•ì¸ í•„ìš”, ì¶”ìƒì ì¸ ë‹µë³€, ê²°ë¡ ì´ ë¶ˆëª…í™•í•¨ ë“±

**ìƒì„¸ ë¶„ì„ ê¸°ì¤€**:
- í‰ê°€: ì¢‹ìŒ/ë³´í†µ/ë‚˜ì¨ (ë‹µë³€ì˜ ì¶©ì‹¤ë„, êµ¬ì²´ì„±, ë…¼ë¦¬ì„± ê³ ë ¤)
- ê°œì„  í¬ì¸íŠ¸: "ë‚´ ì—­í• ì„ ë” ëª…í™•íˆ ê°•ì¡°í•˜ê¸°", "ê²°ë¡ ì„ ë¨¼ì € ë§í•˜ê³  êµ¬ì²´ ì‚¬ë¡€ ë§ë¶™ì´ê¸°" ë“±
- ë³´ì™„ í•„ìš”: "ë°°ìš´ ì ì„ ì „ê³µê³¼ ì—°ê²°í•˜ëŠ” ë¬¸ì¥ 1ì¤„ ì¶”ê°€", "êµ¬ì²´ì ì¸ ê²°ê³¼ ìˆ˜ì¹˜ ì–¸ê¸‰í•˜ê¸°" ë“±

**JSON í˜•ì‹ìœ¼ë¡œ ì¢…í•© í‰ê°€ë¥¼ ìƒì„±í•˜ì„¸ìš”.**

ê° ë‹µë³€ì— ëŒ€í•´ ì§ˆë¬¸ ë‚´ìš©, ë‹µë³€ ì‹œê°„, í‰ê°€, ê°œì„  í¬ì¸íŠ¸, ë³´ì™„ í•„ìš” í•­ëª©ì„ ë¶„ì„í•˜ì„¸ìš”."""

            # 7. JSON ìŠ¤í‚¤ë§ˆ
            schema = self.types.Schema(
                type=self.types.Type.OBJECT,
                properties={
                    "scores": self.types.Schema(
                        type=self.types.Type.OBJECT,
                        properties={
                            "ì „ê³µì í•©ì„±": self.types.Schema(type=self.types.Type.INTEGER, minimum=0, maximum=25),
                            "ì¸ì„±": self.types.Schema(type=self.types.Type.INTEGER, minimum=0, maximum=25),
                            "ë°œì „ê°€ëŠ¥ì„±": self.types.Schema(type=self.types.Type.INTEGER, minimum=0, maximum=25),
                            "ì˜ì‚¬ì†Œí†µëŠ¥ë ¥": self.types.Schema(type=self.types.Type.INTEGER, minimum=0, maximum=25),
                            "ì´ì ": self.types.Schema(type=self.types.Type.INTEGER, minimum=0, maximum=100)
                        },
                        required=["ì „ê³µì í•©ì„±", "ì¸ì„±", "ë°œì „ê°€ëŠ¥ì„±", "ì˜ì‚¬ì†Œí†µëŠ¥ë ¥", "ì´ì "]
                    ),
                    "strength_tags": self.types.Schema(
                        type=self.types.Type.ARRAY,
                        items=self.types.Schema(type=self.types.Type.STRING)
                    ),
                    "weakness_tags": self.types.Schema(
                        type=self.types.Type.ARRAY,
                        items=self.types.Schema(type=self.types.Type.STRING)
                    ),
                    "detailed_analysis": self.types.Schema(
                        type=self.types.Type.ARRAY,
                        items=self.types.Schema(
                            type=self.types.Type.OBJECT,
                            properties={
                                "question": self.types.Schema(type=self.types.Type.STRING, description="ì§ˆë¬¸ ë‚´ìš©"),
                                "response_time": self.types.Schema(type=self.types.Type.INTEGER, description="ë‹µë³€ ì‹œê°„(ì´ˆ)"),
                                "evaluation": self.types.Schema(type=self.types.Type.STRING, description="í‰ê°€ (ì¢‹ìŒ/ë³´í†µ/ë‚˜ì¨)"),
                                "improvement_point": self.types.Schema(type=self.types.Type.STRING, description="ê°œì„  í¬ì¸íŠ¸"),
                                "supplement_needed": self.types.Schema(type=self.types.Type.STRING, description="ë³´ì™„ í•„ìš” ì‚¬í•­")
                            },
                            required=["question", "response_time", "evaluation", "improvement_point", "supplement_needed"]
                        )
                    )
                },
                required=["scores", "strength_tags", "weakness_tags", "detailed_analysis"]
            )

            # 8. Gemini í˜¸ì¶œ
            response = self.client.models.generate_content(
                model=self.model,
                contents=prompt,
                config={
                    "response_mime_type": "application/json",
                    "response_json_schema": schema,
                }
            )

            result = json.loads(response.text)

            # 9. ê²°ê³¼ ë°˜í™˜
            return {
                "scores": result.get("scores", {}),
                "strength_tags": result.get("strength_tags", []),
                "weakness_tags": result.get("weakness_tags", []),
                "detailed_analysis": result.get("detailed_analysis", [])
            }

        except Exception as e:
            logger.error(f"Error analyzing interview result: {e}")
            return {
                "error": str(e),
                "message": "ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            }


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
interview_graph = InterviewGraph()
