"""PDF ë²¡í„°í™” ì„œë¹„ìŠ¤ - Gemini ê¸°ë°˜ ì¹´í…Œê³ ë¦¬ë³„ ì²­í‚¹ & Embedding"""
import logging
import io
import json
import fitz  # PyMuPDF
from typing import List, Dict, Tuple
from pydantic import BaseModel
from app.models import RecordChunk
from sqlalchemy.orm import Session

logger = logging.getLogger(__name__)


class VectorService:
    """PDF ë²¡í„°í™” ì„œë¹„ìŠ¤ - Gemini ê¸°ë°˜ ì¹´í…Œê³ ë¦¬ë³„ ì²­í‚¹ & Embedding"""
    
    def __init__(self):
        # google.genai í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        from google import genai
        from google.genai import types
        from config import settings

        self.client = genai.Client(api_key=settings.google_api_key)
        self.types = types
        self.embedding_model = 'text-embedding-004'  # 768ì°¨ì› embedding ëª¨ë¸
        self.chat_model = 'gemini-2.5-flash'  # ì²­í‚¹ìš© ëª¨ë¸
    
    async def vectorize_pdf(
        self,
        pdf_bytes: io.BytesIO,
        record_id: int,
        db: Session,
        progress_callback = None
    ) -> Tuple[bool, str, int]:
        """
        PDFë¥¼ Geminië¡œ ì²­í‚¹í•˜ê³  ë²¡í„°í™”í•˜ì—¬ DB ì €ì¥

        Args:
            pdf_bytes: PDF íŒŒì¼ ë°”ì´íŠ¸ (io.BytesIO)
            record_id: ìƒê¸°ë¶€ ID
            db: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
            progress_callback: ì§„í–‰ë¥  ì½œë°± í•¨ìˆ˜ (progress: int, message: str) -> None


        Returns:
            (ì„±ê³µ ì—¬ë¶€, ë©”ì‹œì§€, ì „ì²´ ì²­í¬ ìˆ˜)
        """
        try:
            logger.info(f"Starting PDF vectorization for record {record_id}")

            # PDF í¬ê¸° í™•ì¸
            pdf_bytes.seek(0)
            pdf_size = len(pdf_bytes.read())
            pdf_bytes.seek(0)

            # 1. PDFë¥¼ 2í˜ì´ì§€ì”© ë°°ì¹˜ë¡œ ë¶„í• 
            # PDF ì „ì²´ë¥¼ fitzë¡œ ì—´ì–´ í˜ì´ì§€ ìˆ˜ í™•ì¸
            import fitz
            doc = fitz.open(stream=pdf_bytes.read(), filetype="pdf")
            total_pages = len(doc)
            doc.close()
            pdf_bytes.seek(0)  # ë‹¤ì‹œ ì²˜ìŒìœ¼ë¡œ



            batch_size = 4  # 4í˜ì´ì§€ì”© ë°°ì¹˜
            total_batches = (total_pages + batch_size - 1) // batch_size

            logger.info(f"PDF split into {total_batches} batches (batch_size={batch_size}, total_pages={total_pages})")

            if progress_callback:
                await progress_callback(30)

            # 2. ê° ë°°ì¹˜ë¥¼ Geminië¡œ íŒŒì‹±
            all_chunks = []
            failed_batches = []

            logger.info("Starting Gemini AI chunking")

            for i in range(total_batches):
                try:
                    start_page = i * batch_size
                    end_page = min(start_page + batch_size, total_pages)
                    pages_in_batch = list(range(start_page, end_page))

                    chunks = await self._parse_pdf_batch_with_gemini(pdf_bytes, pages_in_batch, i, total_batches)

                    if chunks:
                        all_chunks.extend(chunks)
                        logger.info(f"Batch {i+1}/{total_batches}: {len(chunks)} chunks created")
                    else:
                        logger.warning(f"Batch {i+1}: No chunks returned")
                        failed_batches.append(i+1)

                    # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ (30-70%)
                    if progress_callback:
                        batch_progress = 30 + int(((i + 1) / total_batches) * 40)
                        await progress_callback(batch_progress)

                except Exception as e:
                    logger.error(f"Batch {i+1} parsing failed: {e}")
                    failed_batches.append(i+1)

                    # ê³„ì† ì§„í–‰ (í•˜ë‚˜ì˜ ë°°ì¹˜ ì‹¤íŒ¨ê°€ ì „ì²´ë¥¼ ë§ì¹˜ì§€ ì•Šê²Œ)
                    if progress_callback:
                        batch_progress = 30 + int(((i + 1) / total_batches) * 40)
                        await progress_callback(batch_progress)
                    continue

            # ì‹¤íŒ¨í•œ ë°°ì¹˜ê°€ ìˆìœ¼ë©´ ì „ì²´ ì‹¤íŒ¨ ì²˜ë¦¬
            if failed_batches:
                logger.error(f"Batch parsing failed: {failed_batches}")
                return False, f"Batch parsing failed: {failed_batches}", 0

            if not all_chunks:
                logger.error("No chunks generated")
                return False, "Failed to generate chunks", 0

            # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
            category_counts = {}
            for chunk in all_chunks:
                cat = chunk['category']
                category_counts[cat] = category_counts.get(cat, 0) + 1
            logger.info(f"Generated {len(all_chunks)} chunks total: {category_counts}")

            # 3. ê° ì²­í¬ë¥¼ ë²¡í„°í™”í•˜ê³  ì €ì¥
            if progress_callback:
                await progress_callback(75)

            logger.info(f"Starting embedding and DB save for {len(all_chunks)} chunks")

            saved_count = 0
            for chunk_data in all_chunks:
                try:
                    # í…ìŠ¤íŠ¸ ì„ë² ë”©
                    embedding = await self._embed_text(chunk_data['text'])

                    # DB ì €ì¥
                    chunk = RecordChunk(
                        record_id=record_id,
                        chunk_text=chunk_data['text'],
                        chunk_index=chunk_data['index'],
                        category=chunk_data['category'],
                        embedding=embedding  # pgvector Vector íƒ€ì…ì— ë¦¬ìŠ¤íŠ¸ ì§ì ‘ ì „ë‹¬
                    )
                    db.add(chunk)
                    saved_count += 1

                    # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ (75-95%)
                    if progress_callback:
                        embed_progress = 75 + int((saved_count / len(all_chunks)) * 20)
                        await progress_callback(embed_progress)

                except Exception as e:
                    logger.error(f"Chunk {chunk_data['index'] + 1} processing failed: {e}")
                    continue

            db.commit()

            logger.info(f"PDF vectorization completed: {saved_count} chunks saved for record {record_id}")

            # ì €ì¥ëœ ì²­í¬ê°€ ì—†ìœ¼ë©´ ì‹¤íŒ¨ ë°˜í™˜
            if saved_count == 0:
                logger.error("No chunks were vectorized")
                return False, "No chunks were vectorized", 0

            return True, f"{saved_count} chunks vectorized", saved_count

        except Exception as e:
            logger.error(f"PDF vectorization failed: {str(e)}")
            db.rollback()
            return False, f"Vectorization error: {str(e)}", 0
    
    async def _parse_pdf_batch_with_gemini(
        self,
        pdf_bytes: io.BytesIO,
        page_numbers: List[int],
        batch_index: int,
        total_batches: int
    ) -> List[Dict]:
        """
        Gemini 2.5 Flashë¡œ PDF í˜ì´ì§€ ë°°ì¹˜ë¥¼ íŒŒì‹±
        
        Args:
            pdf_bytes: PDF íŒŒì¼ ë°”ì´íŠ¸
            page_numbers: ì²˜ë¦¬í•  í˜ì´ì§€ ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸ (0-based)
            batch_index: ë°°ì¹˜ ì¸ë±ìŠ¤
            total_batches: ì „ì²´ ë°°ì¹˜ ìˆ˜
            
        Returns:
            ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        import json
        import fitz  # PyMuPDF
        
        prompt = """ë‹¹ì‹ ì€ í•™êµ ìƒí™œê¸°ë¡ë¶€ ì „ë¬¸ ë¶„ì„ê°€ì…ë‹ˆë‹¤.

PDF íŒŒì¼ì€ í•™ìƒì˜ ìƒí™œê¸°ë¡ë¶€ì…ë‹ˆë‹¤. ê° í˜ì´ì§€ì˜ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ì²­í‚¹í•˜ê³  JSON í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”.

## ì²­í‚¹ ê·œì¹™ (ì¤‘ìš”)

1. **ê°œì¸ì •ë³´ ì™„ì „ ì‚­ì œ**: ì´ë¦„ â†’ [ì´ë¦„], ë²ˆí˜¸ â†’ [ë²ˆí˜¸], ì£¼ì†Œ â†’ [ì£¼ì†Œ]
2. **ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜**: ì„±ì , ì„¸íŠ¹, ì°½ì²´, í–‰íŠ¹, ê¸°íƒ€ ì¤‘ í•˜ë‚˜
3. **ì²­í¬ í¬ê¸°**: í•˜ë‚˜ì˜ contentëŠ” 400~600ì ì´ë‚´ë¡œ êµ¬ì„±
4. **ì¹´í…Œê³ ë¦¬ë³„ í†µí•©**: ê°™ì€ ì¹´í…Œê³ ë¦¬ì˜ í™œë™ë“¤ì€ **í•˜ë‚˜ì˜ contentì— ëª¨ë‘ ë¬¶ì–´ì„œ ì‘ì„±**í•˜ì„¸ìš”. ê° í™œë™ì€ " | "ë¡œ êµ¬ë¶„í•©ë‹ˆë‹¤.
   - ì˜ˆ: "í™œë™1 ë‚´ìš© | í™œë™2 ë‚´ìš© | í™œë™3 ë‚´ìš©"
5. **ì²­í¬ ë¶„ë¦¬ ê¸°ì¤€**:
   - ê°™ì€ ì¹´í…Œê³ ë¦¬ ë‚´ì—ì„œ 600ìë¥¼ ë„˜ì–´ê°€ë©´ **ê·¸ ì§€ì ì—ì„œ ìƒˆë¡œìš´ ì²­í¬**ë¡œ ë¶„ë¦¬
   - í•œ í™œë™ì´ ë„ˆë¬´ ê¸¸ì–´ì„œ 600ìë¥¼ ë„˜ì„ ê²ƒ ê°™ìœ¼ë©´, **ê·¸ í™œë™ ì „ì²´ë¥¼ ë‹¤ìŒ ì²­í¬ë¡œ** ë„˜ê¸°ì„¸ìš”
   - ì˜ˆ: ì²­í¬1 = "í™œë™1 | í™œë™2 | í™œë™3" (550ì), ì²­í¬2 = "í™œë™4 | í™œë™5" (580ì)
6. **ë‹¨ìˆœ í…ìŠ¤íŠ¸ ë³€í™˜**: í‘œ í˜•ì‹ì˜ ë°ì´í„°(ìˆ˜ìƒê²½ë ¥, ì„±ì  ë“±)ëŠ” ê°„ë‹¨í•œ ë¬¸ì¥ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
7. **ê³µë°± ìµœì†Œí™”**: ë¶ˆí•„ìš”í•œ ì¤„ë°”ê¿ˆ, ê³µë°± ì œê±°í•˜ê³  ê°„ê²°í•˜ê²Œ ì‘ì„±

## ğŸš¨ ì¤‘ìš”: ë°˜ë³µ ì ˆëŒ€ ê¸ˆì§€

- **ê°™ì€ ë¬¸ì¥ ë°˜ë³µ ê¸ˆì§€**: ê°™ì€ ë‚´ìš©ì„ ë°˜ë³µí•´ì„œ ì‘ì„±í•˜ì§€ ë§ˆì„¸ìš”.
- **ë£¨í”„ ë°©ì§€**: í…ìŠ¤íŠ¸ê°€ ë°˜ë³µë˜ëŠ” íŒ¨í„´ì— ë¹ ì§€ì§€ ë§ê³ , ê° í•­ëª©ì„ í•œ ë²ˆì”©ë§Œ ì‘ì„±í•˜ì„¸ìš”.

## ì¶œë ¥ í˜•ì‹

ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”:

```json
{
  "records": [
    {
      "category": "ì°½ì²´",
      "content": "ì¬ë‚œì•ˆì „êµìœ¡ ì°¸ì—¬ | êµë‚´ì²´ìœ¡í–‰ì‚¬ ë†êµ¬, 2ì¸3ê°, ì¤„ë‹¤ë¦¬ê¸° ì°¸ì—¬ | í•™êµí­ë ¥ì˜ˆë°©êµìœ¡ ì´ìˆ˜ ë° ìº í˜ì¸ í™œë™ | ë…ë„ êµìœ¡ ë° SNS ìº í˜ì¸ ì°¸ì—¬ | ìˆ˜í•™ì—¬í–‰ ì œì£¼ë„ ì²´í—˜ ë° 4.3í‰í™”ê³µì› ê´€ëŒ"
    },
    {
      "category": "ì„¸íŠ¹",
      "content": "English Conversation ì—­í• ê·¹ í™œë™ | ì•Œê³ ë¦¬ì¦˜ ì—°êµ¬ë°˜ ë¬¸ì œ í•´ê²° ë° í”„ë¡œê·¸ë¨ ì‘ì„±"
    }
  ]
}
```

## ì ˆëŒ€ ê¸ˆì§€ ì‚¬í•­

- **í™œë™ë³„ë¡œ ë”°ë¡œë”°ë¡œ ì²­í¬ ë§Œë“¤ì§€ ë§ˆì„¸ìš”**: ê°™ì€ ì¹´í…Œê³ ë¦¬ëŠ” ë°˜ë“œì‹œ í•˜ë‚˜ì— ë¬¶ì–´ì£¼ì„¸ìš”
- **ë¶ˆí•„ìš”í•œ í˜•ì‹ ì œê±°**: ë§ˆí¬ë‹¤ìš´ í‘œ, ì—¬ëŸ¬ ì¤„ë°”ê¿ˆ ì œê±°
- **ë‚´ìš©ì„ ìš”ì•½/ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”**: PDFì— ìˆëŠ” í…ìŠ¤íŠ¸ë§Œ ìˆëŠ” ê·¸ëŒ€ë¡œ ì¶”ì¶œí•˜ì„¸ìš”
- **ê°™ì€ ë‚´ìš© ë°˜ë³µ ê¸ˆì§€**: ê°™ì€ ë¬¸ì¥ì´ë‚˜ ë‹¨ë½ì„ 2ë²ˆ ì´ìƒ ë°˜ë³µí•˜ì§€ ë§ˆì„¸ìš”
- **JSON ì™¸ì˜ í…ìŠ¤íŠ¸ ì¶œë ¥ ê¸ˆì§€**: ì„¤ëª…ì´ë‚˜ ë¶„ì„ ì—†ì´ JSONë§Œ ë°˜í™˜í•˜ì„¸ìš”"""
        
        try:
            # PDFì—ì„œ í•´ë‹¹ í˜ì´ì§€ ì¶”ì¶œ
            pdf_bytes.seek(0)
            doc = fitz.open(stream=pdf_bytes.read(), filetype="pdf")
            
            # ê° í˜ì´ì§€ë¥¼ ê°œë³„ PDFë¡œ ë³€í™˜
            import io
            pdf_parts = []
            for page_num in page_numbers:
                page = doc[page_num]
                # ë‹¨ì¼ í˜ì´ì§€ PDF ìƒì„±
                single_page_doc = fitz.open()
                single_page_doc.insert_pdf(doc, from_page=page_num, to_page=page_num)
                
                # ë°”ì´íŠ¸ë¡œ ë³€í™˜
                pdf_byte_arr = io.BytesIO()
                single_page_doc.save(pdf_byte_arr, garbage=4, deflate=True)
                pdf_bytes_data = pdf_byte_arr.getvalue()
                single_page_doc.close()
                
                # genai.Partë¡œ ë³€í™˜
                pdf_parts.append(self.types.Part.from_bytes(
                    data=pdf_bytes_data,
                    mime_type="application/pdf"
                ))
            
            doc.close()

            # Pydantic ìŠ¤í‚¤ë§ˆ ì •ì˜ (Structured Output)
            class Record(BaseModel):
                category: str
                content: str

            class ResponseList(BaseModel):
                records: list[Record]

            # Gemini 2.5 Flashì— ìš”ì²­ ì „ì†¡ (Structured Outputìœ¼ë¡œ ê°•ì œ)
            response = self.client.models.generate_content(
                model=self.chat_model,
                contents=[prompt] + pdf_parts,
                config={
                    "response_mime_type": "application/json",
                    "response_json_schema": ResponseList.model_json_schema(),
                    "temperature": 0.7  # ì¤‘ê°„ temperatureë¡œ ë°˜ë³µ ë°©ì§€ + ì°½ì˜ì„± ìœ ì§€
                }
            )
            
            # ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° JSON íŒŒì‹±
            response_text = response.text
            logger.info(f"Gemini response received: {len(response_text)} characters")
            
            result = json.loads(response_text)
            
            records = result.get('records', [])
            logger.info(f"Extracted {len(records)} chunks from batch {batch_index + 1}")
            
            
            # RecordChunk í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            chunks = []
            for i, record in enumerate(records):
                chunks.append({
                    'index': i,
                    'text': record['content'],
                    'category': record['category']
                })
            
            return chunks
            
        except json.JSONDecodeError as e:
            logger.error(f"JSON parsing failed: {e}, response length: {len(response_text)}")
            raise

        except Exception as e:
            logger.error(f"Gemini processing error: {e}")
            raise

            # Gemini 2.5 Flashì— ìš”ì²­ ì „ì†¡ (Structured Outputìœ¼ë¡œ ê°•ì œ)
            logger.info(f"   ğŸš€ Gemini API ìš”ì²­ ì „ì†¡ ì¤‘... (ì´ ë¶€ë¶„ì—ì„œ ì‹œê°„ ì†Œìš”ë  ìˆ˜ ìˆìŒ)")
            response = self.client.models.generate_content(
                model=self.chat_model,
                contents=[prompt] + image_parts,
                config={
                    "response_mime_type": "application/json",
                    "response_json_schema": ResponseList.model_json_schema(),
                    "temperature": 0.7  # ì¤‘ê°„ temperatureë¡œ ë°˜ë³µ ë°©ì§€ + ì°½ì˜ì„± ìœ ì§€
                }
            )
            logger.info(f"   âœ… Gemini API ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ")
            
            # ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° JSON íŒŒì‹±
            logger.debug(f"   ğŸ“ ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...")
            response_text = response.text
            logger.debug(f"   âœ… ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ")

            # ë””ë²„ê¹…ìš© ì‘ë‹µ ìš”ì•½ ë¡œê·¸
            logger.info(f"   âœ… Gemini ì‘ë‹µ ìˆ˜ì‹ : {len(response_text)}ì")
            logger.debug(f"   ì „ì²´ ì‘ë‹µ:\n{response_text}")
            
            result = json.loads(response_text)
            
            records = result.get('records', [])
            logger.info(f"Extracted {len(records)} chunks from batch {batch_index + 1}")
            
            
            # RecordChunk í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            chunks = []
            for i, record in enumerate(records):
                chunks.append({
                    'index': i,
                    'text': record['content'],
                    'category': record['category']
                })
            
            return chunks
            
        except json.JSONDecodeError as e:
            logger.error("")
            logger.error("âŒ JSON íŒŒì‹± ì‹¤íŒ¨")
            logger.error(f"   ì—ëŸ¬: {e}")
            logger.error(f"   ì‘ë‹µ ê¸¸ì´: {len(response_text)}ì")
            logger.error(f"   ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸°: {response_text[:300]}...")
            logger.error("=" * 60)

            # JSONì´ ë¶ˆì™„ì „í•œ ê²½ìš° ë³µêµ¬ ì‹œë„
            try:
                # ë§ˆì§€ë§‰ ]} ë¡œ ëë‚˜ì§€ ì•Šìœ¼ë©´ ì¶”ê°€
                if not response_text.rstrip().endswith("}]}"):
                    logger.warning("JSON appears incomplete, attempting to fix...")

                    # ë§ˆì§€ë§‰ ì™„ì „í•œ ë ˆì½”ë“œ ì°¾ê¸° ì‹œë„
                    last_record_end = response_text.rfind("}")
                    if last_record_end > 0:
                        fixed_json = response_text[:last_record_end+1] + "\n  ]\n}"
                        logger.info(f"Attempting to parse fixed JSON (length: {len(fixed_json)})")

                        result = json.loads(fixed_json)
                        records = result.get('records', [])

                        if records:
                            logger.info(f"Successfully recovered {len(records)} records from incomplete JSON")
                            chunks = []
                            for i, record in enumerate(records):
                                chunks.append({
                                    'index': i,
                                    'text': record['content'],
                                    'category': record['category']
                                })
                            return chunks
            except Exception as fix_error:
                logger.debug(f"   JSON ë³µêµ¬ ì‹œë„ ì‹¤íŒ¨: {fix_error}")

            raise
        except Exception as e:
            logger.error(f"Gemini processing error: {e}")
            raise
    
    def _pil_image_to_part(self, image):
        """PIL ì´ë¯¸ì§€ë¥¼ Geminiì— ì „ì†¡ ê°€ëŠ¥í•œ Partë¡œ ë³€í™˜"""
        import io
        from google.genai import types
        
        # ì´ë¯¸ì§€ë¥¼ ë°”ì´íŠ¸ë¡œ ë³€í™˜
        img_byte_arr = io.BytesIO()
        image.save(img_byte_arr, format='PNG')
        img_bytes = img_byte_arr.getvalue()
        
        # genai.Partë¡œ ë³€í™˜
        return types.Part.from_bytes(
            data=img_bytes,
            mime_type="image/png"
        )
    
    async def _embed_text(self, text: str) -> List[float]:
        """í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ì„ë² ë”©"""
        try:
            result = self.client.models.embed_content(
                model=self.embedding_model,
                contents=text
            )
            return result.embeddings[0].values
        except Exception as e:
            logger.error(f"Embedding failed: {e}")
            raise
    
    async def search_chunks_by_topic(
        self,
        record_id: int,
        topic: str
    ) -> List[Dict]:
        """
        ì£¼ì œì— ë”°ë¼ ê´€ë ¨ ì²­í¬ ê²€ìƒ‰
        
        Args:
            record_id: ìƒê¸°ë¶€ ID
            topic: í•˜ìœ„ ì£¼ì œ (ì¶œê²°, ì„±ì , ë™ì•„ë¦¬, ë¦¬ë”ì‹­, ì¸ì„±/íƒœë„, ì§„ë¡œ/ììœ¨, ë…ì„œ, ë´‰ì‚¬)
        
        Returns:
            ê´€ë ¨ ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        try:
            from app.database import get_db
            from app.models import RecordChunk
            
            # DB ì„¸ì…˜ ìƒì„±
            db_generator = get_db()
            db = next(db_generator)
            
            try:
                # ì£¼ì œë³„ ì¹´í…Œê³ ë¦¬ ë§¤í•‘
                topic_category_map = {
                    "ì¶œê²°": "ê¸°íƒ€",
                    "ì„±ì ": "ì„±ì ",
                    "ë™ì•„ë¦¬": "ì°½ì²´",
                    "ë¦¬ë”ì‹­": "í–‰íŠ¹",
                    "ì¸ì„±/íƒœë„": "í–‰íŠ¹",
                    "ì§„ë¡œ/ììœ¨": "ì„¸íŠ¹",
                    "ë…ì„œ": "ì„¸íŠ¹",
                    "ë´‰ì‚¬": "ì°½ì²´"
                }
                
                category = topic_category_map.get(topic, "ê¸°íƒ€")
                
                # í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì˜ ì²­í¬ ì¡°íšŒ
                chunks = db.query(RecordChunk).filter(
                    RecordChunk.record_id == record_id,
                    RecordChunk.category == category
                ).order_by(RecordChunk.chunk_index).all()
                
                # ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë³€í™˜
                result = [
                    {
                        "text": chunk.chunk_text,
                        "category": chunk.category,
                        "chunk_index": chunk.chunk_index
                    }
                    for chunk in chunks
                ]
                
                logger.info(f"Retrieved {len(result)} chunks for topic {topic} (category: {category})")
                return result
                
            finally:
                db.close()
                
        except Exception as e:
            logger.error(f"Error searching chunks for topic {topic}: {e}")
            return []


vector_service = VectorService()
