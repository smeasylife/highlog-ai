"""PDF ë²¡í„°í™” ì„œë¹„ìŠ¤ - Gemini ê¸°ë°˜ ì¹´í…Œê³ ë¦¬ë³„ ì²­í‚¹ & Embedding"""
import logging
import io
import json
import fitz  # PyMuPDF
import asyncio
from typing import List, Dict, Tuple
from pydantic import BaseModel
from app.models import RecordChunk
from sqlalchemy.orm import Session
from sqlalchemy import text

logger = logging.getLogger(__name__)


class RecordData(BaseModel):
    """ìƒí™œê¸°ë¡ë¶€ ì²­í¬ ë°ì´í„° ëª¨ë¸"""
    category: str
    content: str


class RecordsResponse(BaseModel):
    """ìƒí™œê¸°ë¡ë¶€ ì‘ë‹µ ëª¨ë¸"""
    records: List[RecordData]


class VectorService:
    """PDF ë²¡í„°í™” ì„œë¹„ìŠ¤ - Gemini ê¸°ë°˜ ì¹´í…Œê³ ë¦¬ë³„ ì²­í‚¹ & Embedding"""

    def __init__(self):
        # google.genai í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        from google import genai
        from google.genai import types
        from config import settings

        self.client = genai.Client(
            api_key=settings.google_api_key
        )
        self.types = types
        self.genai = genai
        self.embedding_model = 'gemini-embedding-001'  # 768ì°¨ì› embedding ëª¨ë¸
        self.chat_model = 'gemini-2.5-flash'  # ì²­í‚¹ìš© ëª¨ë¸
    
    async def vectorize_pdf(
        self,
        pdf_bytes: io.BytesIO,
        record_id: int,
        db: Session,
        progress_callback = None
    ) -> Tuple[bool, str, int]:
        """
        PDFë¥¼ Geminië¡œ ì²­í‚¹í•˜ê³  ë²¡í„°í™”í•˜ì—¬ DB ì €ì¥

        Args:
            pdf_bytes: PDF íŒŒì¼ ë°”ì´íŠ¸ (io.BytesIO)
            record_id: ìƒê¸°ë¶€ ID
            db: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
            progress_callback: ì§„í–‰ë¥  ì½œë°± í•¨ìˆ˜ (progress: int, message: str) -> None


        Returns:
            (ì„±ê³µ ì—¬ë¶€, ë©”ì‹œì§€, ì „ì²´ ì²­í¬ ìˆ˜)
        """
        try:
            logger.info(f"Starting PDF vectorization for record {record_id}")

            # PDF ì „ì²´ë¥¼ fitzë¡œ ì—´ì–´ í˜ì´ì§€ ìˆ˜ í™•ì¸
            import fitz
            doc = fitz.open(stream=pdf_bytes.read(), filetype="pdf")
            total_pages = len(doc)
            doc.close()
            pdf_bytes.seek(0)  # ë‹¤ì‹œ ì²˜ìŒìœ¼ë¡œ

            batch_size = 4  # 4í˜ì´ì§€ì”© ë°°ì¹˜
            total_batches = (total_pages + batch_size - 1) // batch_size

            logger.info(f"ğŸ“„ {total_pages} pages â†’ {total_batches} batches ({batch_size} pages/batch)")
            
            if progress_callback:
                await progress_callback(10)

            # 2. ëª¨ë“  ë°°ì¹˜ë¥¼ ë™ì‹œì— ì²˜ë¦¬ (ë³‘ë ¬ ì²˜ë¦¬) âš¡
            all_chunks = []
            failed_batches = []

            logger.info("ğŸ¤– AI Chunking (Parallel Processing)...")

            # ëª¨ë“  ë°°ì¹˜ íƒœìŠ¤í¬ ìƒì„±
            tasks = []
            for i in range(total_batches):
                start_page = i * batch_size
                end_page = min(start_page + batch_size, total_pages)
                pages_in_batch = list(range(start_page, end_page))
                tasks.append(self._parse_pdf_batch_with_gemini(
                    pdf_bytes, pages_in_batch, i, total_batches
                ))

            # ë™ì‹œ ì‹¤í–‰ (ë³‘ë ¬ ì²˜ë¦¬)
            results = await asyncio.gather(*tasks, return_exceptions=True)

            # ê²°ê³¼ ì§‘ê³„
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    logger.warning(f"âš ï¸  [{i+1}/{total_batches}] Failed: {str(result)[:80]}... - Skipping")
                    failed_batches.append(i + 1)
                elif result:
                    all_chunks.extend(result)
                    start_page = i * batch_size
                    end_page = min(start_page + batch_size, total_pages)
                    logger.info(f"ğŸ“¦ [{i+1}/{total_batches}] {len(result)} chunks (pages {start_page+1}-{end_page})")
                else:
                    logger.warning(f"âš ï¸  [{i+1}/{total_batches}] No chunks")
                    failed_batches.append(i + 1)

            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
            if progress_callback:
                await progress_callback(70)

            # ì‹¤íŒ¨í•œ ë°°ì¹˜ê°€ ìˆì–´ë„ ê³„ì† ì§„í–‰ (ë¶€ë¶„ ì„±ê³µ í—ˆìš©)
            if failed_batches:
                logger.warning(f"âš ï¸ Some batches failed: {failed_batches} - but continuing with {len(all_chunks)} chunks")

            if not all_chunks:
                logger.error("No chunks generated from any batch")
                return False, "Failed to generate chunks from all batches", 0

            # 3. ë°°ì¹˜ ì„ë² ë”© & ë²Œí¬ DB ì‚½ì… ğŸ”¥
            if progress_callback:
                await progress_callback(75)

            logger.info(f"ğŸ”„ Batch Embedding {len(all_chunks)} chunks...")

            # ë°°ì¹˜ ì„ë² ë”© (20ê°œì”©)
            batch_size = 20
            all_embeddings = []
            failed_embeddings = 0

            for i in range(0, len(all_chunks), batch_size):
                batch = all_chunks[i:i+batch_size]
                texts = [chunk['text'] for chunk in batch]
                
                try:
                    embeddings = await self._embed_batch(texts)
                    all_embeddings.extend(embeddings)
                    
                    # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ (75-90%)
                    if progress_callback:
                        embed_progress = 75 + int(((i + batch_size) / len(all_chunks)) * 15)
                        await progress_callback(min(embed_progress, 90))
                        
                except Exception as e:
                    logger.warning(f"âš ï¸  Batch {i//batch_size + 1} embedding failed: {str(e)[:50]}")
                    # ì‹¤íŒ¨í•œ ë°°ì¹˜ëŠ” ê°œë³„ ì„ë² ë”©ìœ¼ë¡œ ì‹œë„
                    for chunk in batch:
                        try:
                            emb = await self._embed_text(chunk['text'])
                            all_embeddings.append(emb)
                        except Exception as e2:
                            logger.debug(f"   âŒ Individual chunk failed: {str(e2)[:50]}")
                            all_embeddings.append(None)  # ì‹¤íŒ¨ í‘œì‹œ
                            failed_embeddings += 1

            # 4. ë²Œí¬ DB ì‚½ì… (í•œ ë²ˆì— ì €ì¥) ğŸš€
            logger.info("ğŸ’¾ Bulk inserting to database...")
            
            bulk_data = []
            for idx, chunk_data in enumerate(all_chunks):
                if idx < len(all_embeddings) and all_embeddings[idx] is not None:
                    bulk_data.append({
                        'record_id': record_id,
                        'chunk_text': chunk_data['text'],
                        'chunk_index': chunk_data['index'],
                        'category': chunk_data['category'],
                        'embedding': all_embeddings[idx]
                    })
            
            if bulk_data:
                db.bulk_insert_mappings(RecordChunk, bulk_data)
                db.commit()

            saved_count = len(bulk_data)

            # ìµœì¢… ìš”ì•½ í•œ ì¤„ë¡œ
            result_parts = [f"âœ… {saved_count} saved"]
            if failed_batches:
                result_parts.append(f"{len(failed_batches)} batches failed")
            if failed_embeddings:
                result_parts.append(f"{failed_embeddings} embeddings failed")
            
            logger.info("ğŸ“Š " + ", ".join(result_parts))

            # ì €ì¥ëœ ì²­í¬ê°€ 1ê°œ ì´ìƒì´ë©´ ì„±ê³µ (ë¶€ë¶„ ì„±ê³µ í—ˆìš©)
            if saved_count == 0:
                logger.error("âŒ No chunks were successfully vectorized")
                return False, "No chunks were successfully vectorized", 0

            # ì„±ê³µ ë©”ì‹œì§€ì— ì‹¤íŒ¨ ì •ë³´ í¬í•¨
            success_msg = f"{saved_count} chunks successfully vectorized"
            if failed_batches:
                success_msg += f" ({len(failed_batches)} batches failed but skipped)"
            if failed_embeddings:
                success_msg += f" ({failed_embeddings} chunks failed to embed)"

            return True, success_msg, saved_count

        except Exception as e:
            logger.error(f"PDF vectorization failed: {str(e)}")
            db.rollback()
            return False, f"Vectorization error: {str(e)}", 0
    
    async def _parse_pdf_batch_with_gemini(
        self,
        pdf_bytes: io.BytesIO,
        page_numbers: List[int],
        batch_index: int,
        total_batches: int
    ) -> List[Dict]:
        """
        Gemini 2.5 Flashë¡œ PDF í˜ì´ì§€ ë°°ì¹˜ë¥¼ íŒŒì‹±
        
        Args:
            pdf_bytes: PDF íŒŒì¼ ë°”ì´íŠ¸
            page_numbers: ì²˜ë¦¬í•  í˜ì´ì§€ ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸ (0-based)
            batch_index: ë°°ì¹˜ ì¸ë±ìŠ¤
            total_batches: ì „ì²´ ë°°ì¹˜ ìˆ˜
            
        Returns:
            ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        import json
        import fitz  # PyMuPDF
        
        prompt = """ë‹¹ì‹ ì€ í•™êµ ìƒí™œê¸°ë¡ë¶€ ì „ë¬¸ ë¶„ì„ê°€ì…ë‹ˆë‹¤.

PDF íŒŒì¼ì€ í•™ìƒì˜ ìƒí™œê¸°ë¡ë¶€ì…ë‹ˆë‹¤. ê° í˜ì´ì§€ì˜ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ì²­í‚¹í•˜ê³  JSON í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”.

## ì²­í‚¹ ê·œì¹™ (ì¤‘ìš”)

1. **ê°œì¸ì •ë³´ ì™„ì „ ì‚­ì œ**: ì´ë¦„ â†’ [ì´ë¦„], ë²ˆí˜¸ â†’ [ë²ˆí˜¸], ì£¼ì†Œ â†’ [ì£¼ì†Œ]
2. **ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜**: ì„±ì , ì„¸íŠ¹, ì°½ì²´, í–‰íŠ¹, ê¸°íƒ€ ì¤‘ í•˜ë‚˜
3. **ì²­í¬ í¬ê¸°**: í•˜ë‚˜ì˜ contentëŠ” 400~600ì ì´ë‚´ë¡œ êµ¬ì„±
4. **ì¹´í…Œê³ ë¦¬ë³„ í†µí•©**: ê°™ì€ ì¹´í…Œê³ ë¦¬ì˜ í™œë™ë“¤ì€ **í•˜ë‚˜ì˜ contentì— ëª¨ë‘ ë¬¶ì–´ì„œ ì‘ì„±**í•˜ì„¸ìš”. ê° í™œë™ì€ " | "ë¡œ êµ¬ë¶„í•©ë‹ˆë‹¤.
   - ì˜ˆ: "í™œë™1 ë‚´ìš© | í™œë™2 ë‚´ìš© | í™œë™3 ë‚´ìš©"
5. **ì²­í¬ ë¶„ë¦¬ ê¸°ì¤€**:
   - ê°™ì€ ì¹´í…Œê³ ë¦¬ ë‚´ì—ì„œ 600ìë¥¼ ë„˜ì–´ê°€ë„ ì¢‹ìœ¼ë‹ˆ ê·¸ ë‚´ìš©ì„ ëê¹Œì§€ ì¶œë ¥í•˜ê³  ë‹¤ìŒ ì²­í¬ë¡œ ë„˜ì–´ê°€ì„¸ìš”. (...ìœ¼ë¡œ ë‚´ìš© ëŠê¸° ê¸ˆì§€)
6. **ë‹¨ìˆœ í…ìŠ¤íŠ¸ ë³€í™˜**: í‘œ í˜•ì‹ì˜ ë°ì´í„°(ìˆ˜ìƒê²½ë ¥, ì„±ì  ë“±)ëŠ” ê°„ë‹¨í•œ ë¬¸ì¥ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì„¸ìš”.
 - ì˜ˆ: 2í•™ë…„ ì§„ë¡œì‚¬í•­/í”„ë¡œê·¸ë˜ë¨¸/ ìŠ¤ë§ˆíŠ¸ì‹œí‹°ì— ê´€ì‹¬ì´ ìˆì—ˆìœ¼ë©° ~, 3í•™ë…„ ì§„ë¡œì‚¬í•­/ ì •ë³´í†µì‹ ë¶„ì•¼ / AIì—ë„ ê´€ì‹¬ì´ ìˆê³  ì„ë² ë””ë“œì—ë„~ 
7. **ê³µë°± ìµœì†Œí™”**: ë¶ˆí•„ìš”í•œ ì¤„ë°”ê¿ˆ, ê³µë°± ì œê±°
8. ë‚´ìš©ì„ ê°„ì†Œí™”í•˜ë ¤ê³  í•˜ê±°ë‚˜ ìš”ì•½í•˜ì§€ ë§ˆì„¸ìš”. ìˆëŠ” ê·¸ëŒ€ë¡œ ì‘ì„±í•˜ì„¸ìš”.

## ğŸš¨ ì¤‘ìš”: ë°˜ë³µ ì ˆëŒ€ ê¸ˆì§€

- **ê°™ì€ ë¬¸ì¥ ë°˜ë³µ ê¸ˆì§€**: ê°™ì€ ë‚´ìš©ì„ ë°˜ë³µí•´ì„œ ì‘ì„±í•˜ì§€ ë§ˆì„¸ìš”.

## ì¶œë ¥ í˜•ì‹

ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”:

```json
{
  "records": [
    {
      "category": "ì°½ì²´",
      "content": "ì¬ë‚œì•ˆì „êµìœ¡ ì°¸ì—¬ | êµë‚´ì²´ìœ¡í–‰ì‚¬ ë†êµ¬, 2ì¸3ê°, ì¤„ë‹¤ë¦¬ê¸° ì°¸ì—¬ | í•™êµí­ë ¥ì˜ˆë°©êµìœ¡ ì´ìˆ˜ ë° ìº í˜ì¸ í™œë™ | ë…ë„ êµìœ¡ ë° SNS ìº í˜ì¸ ì°¸ì—¬ | ìˆ˜í•™ì—¬í–‰ ì œì£¼ë„ ì²´í—˜ ë° 4.3í‰í™”ê³µì› ê´€ëŒ"
    },
    {
      "category": "ì„¸íŠ¹",
      "content": "English Conversation ì—­í• ê·¹ í™œë™ | ì•Œê³ ë¦¬ì¦˜ ì—°êµ¬ë°˜ ë¬¸ì œ í•´ê²° ë° í”„ë¡œê·¸ë¨ ì‘ì„±"
    }
  ]
}
```

## ì ˆëŒ€ ê¸ˆì§€ ì‚¬í•­

- **í™œë™ë³„ë¡œ ë”°ë¡œë”°ë¡œ ì²­í¬ ë§Œë“¤ì§€ ë§ˆì„¸ìš”**: ê°™ì€ ì¹´í…Œê³ ë¦¬ëŠ” ë°˜ë“œì‹œ í•˜ë‚˜ì— ë¬¶ì–´ì£¼ì„¸ìš”
- **ë¶ˆí•„ìš”í•œ í˜•ì‹ ì œê±°**: ë§ˆí¬ë‹¤ìš´ í‘œ, ì—¬ëŸ¬ ì¤„ë°”ê¿ˆ ì œê±°
- **ë‚´ìš©ì„ ìš”ì•½/ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”**: PDFì— ìˆëŠ” í…ìŠ¤íŠ¸ë§Œ ìˆëŠ” ê·¸ëŒ€ë¡œ ì¶”ì¶œí•˜ì„¸ìš”
- **ê°™ì€ ë‚´ìš© ë°˜ë³µ ê¸ˆì§€**: ê°™ì€ ë¬¸ì¥ì´ë‚˜ ë‹¨ë½ì„ 2ë²ˆ ì´ìƒ ë°˜ë³µí•˜ì§€ ë§ˆì„¸ìš”
- **JSON ì™¸ì˜ í…ìŠ¤íŠ¸ ì¶œë ¥ ê¸ˆì§€**: ì„¤ëª…ì´ë‚˜ ë¶„ì„ ì—†ì´ JSONë§Œ ë°˜í™˜í•˜ì„¸ìš”"""
        
        try:
            # PDFì—ì„œ í•´ë‹¹ í˜ì´ì§€ ì¶”ì¶œ
            pdf_bytes.seek(0)
            doc = fitz.open(stream=pdf_bytes.read(), filetype="pdf")

            # ê° í˜ì´ì§€ë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜
            image_parts = []
            for page_num in page_numbers:
                page = doc[page_num]
                # í˜ì´ì§€ë¥¼ ì¤‘ê°„ í•´ìƒë„ ì´ë¯¸ì§€ë¡œ ë³€í™˜ (ì†ë„ì™€ í’ˆì§ˆ ë°¸ëŸ°ìŠ¤)
                pix = page.get_pixmap(dpi=150)
                img_bytes = pix.tobytes("png")

                # genai.Partë¡œ ë³€í™˜
                image_parts.append(self.types.Part.from_bytes(
                    data=img_bytes,
                    mime_type="image/png"
                ))

            doc.close()

            # Gemini 2.5 Flashì— ë¹„ë™ê¸° ìš”ì²­ ì „ì†¡ (JSON í˜•ì‹ ì‘ë‹µ ê°•ì œ)
            logger.info(f"ğŸš€ [{batch_index+1}/{total_batches}] Sending request for pages {page_numbers}...")
            import time
            start_time = time.time()

            response = await self.client.aio.models.generate_content(
                model=self.chat_model,
                contents=[prompt] + image_parts,
                config={
                    "response_mime_type": "application/json",
                    "response_json_schema": RecordsResponse.model_json_schema(),
                }
            )

            elapsed = time.time() - start_time
            logger.info(f"âœ… [{batch_index+1}/{total_batches}] Response received for pages {page_numbers} ({elapsed:.1f}s)")
            
            # ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° JSON íŒŒì‹±
            response_text = response.text
            
            result = json.loads(response_text)
            records = result.get('records', [])
            
            # RecordChunk í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            chunks = []
            for i, record in enumerate(records):
                chunks.append({
                    'index': i,
                    'text': record['content'],
                    'category': record['category']
                })
            
            return chunks
            
        except json.JSONDecodeError as e:
            logger.warning(f"âš ï¸  JSON parsing failed: {str(e)[:50]}")
            raise

        except Exception as e:
            logger.warning(f"âš ï¸  Gemini error: {str(e)}")
            raise

    async def _embed_text(self, text: str) -> List[float]:
        """í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ì„ë² ë”© (768ì°¨ì›) - ê°œë³„ í…ìŠ¤íŠ¸ìš©"""
        try:
            result = await self.client.aio.models.embed_content(
                model=self.embedding_model,
                contents=text,
                config=self.types.EmbedContentConfig(
                    output_dimensionality=768
                )
            )
            return result.embeddings[0].values
        except Exception as e:
            logger.error(f"Embedding failed: {e}")
            raise

    async def _embed_batch(self, texts: List[str]) -> List[List[float]]:
        """ì—¬ëŸ¬ í…ìŠ¤íŠ¸ë¥¼ í•œ ë²ˆì— ë°°ì¹˜ ì„ë² ë”© (768ì°¨ì›) ğŸ”¥

        Google Embedding APIëŠ” ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ì§€ì›í•˜ì—¬ ìµœëŒ€ 100ê°œê¹Œì§€ ë™ì‹œì— ì²˜ë¦¬ ê°€ëŠ¥
        """
        try:
            import time
            start_time = time.time()

            result = await self.client.aio.models.embed_content(
                model=self.embedding_model,
                contents=texts,  # ë¦¬ìŠ¤íŠ¸ ì „ë‹¬
                config=self.types.EmbedContentConfig(
                    output_dimensionality=768
                )
            )

            elapsed = time.time() - start_time
            logger.debug(f"ğŸ“Š Embedded {len(texts)} chunks in {elapsed:.2f}s")

            return [emb.values for emb in result.embeddings]
        except Exception as e:
            logger.warning(f"Batch embedding failed: {e}, falling back to individual")
            # ë°°ì¹˜ ì‹¤íŒ¨ ì‹œ ê°œë³„ ì²˜ë¦¬ë¡œ í´ë°±
            embeddings = []
            for text in texts:
                try:
                    emb = await self._embed_text(text)
                    embeddings.append(emb)
                except Exception as e2:
                    logger.error(f"Individual embedding failed: {e2}")
                    raise
            return embeddings
    
    def search_chunks_by_topic(
        self,
        record_id: int,
        topic: str,
        db: Session = None
    ) -> List[int]:
        """
        ì£¼ì œì— ë”°ë¼ ê´€ë ¨ ì²­í¬ë¥¼ pgvector ìœ ì‚¬ë„ ê²€ìƒ‰ìœ¼ë¡œ ì°¾ê¸°

        Args:
            record_id: ìƒê¸°ë¶€ ID
            topic: í•˜ìœ„ ì£¼ì œ (ì¶œê²°, ì„±ì , ë™ì•„ë¦¬, ë¦¬ë”ì‹­, ì¸ì„±/íƒœë„, ì§„ë¡œ/ììœ¨, ë…ì„œ, ë´‰ì‚¬)
            db: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜ (ì™¸ë¶€ì—ì„œ ì£¼ì…)

        Returns:
            ê´€ë ¨ ì²­í¬ ID ë¦¬ìŠ¤íŠ¸ (ìœ ì‚¬ë„ ìˆœ ìƒìœ„ 3ê°œ)
        """
        try:
            from app.database import get_db

            # DB ì„¸ì…˜ ê°€ì ¸ì˜¤ê¸° (ì™¸ë¶€ì—ì„œ ì£¼ì…ë°›ê±°ë‚˜ ìƒˆë¡œ ìƒì„±)
            if db is None:
                db_generator = get_db()
                db = next(db_generator)
                should_close = True
            else:
                should_close = False

            try:
                # 1. ì£¼ì œë¥¼ embeddingìœ¼ë¡œ ë³€í™˜ (ë™ê¸° ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬)
                query_embedding = self._embed_text_sync(topic)

                # 2. pgvector ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê²€ìƒ‰ (IDë§Œ ë°˜í™˜)
                # <-> ì—°ì‚°ì: ì½”ì‚¬ì¸ ê±°ë¦¬ (ì‘ì„ìˆ˜ë¡ ìœ ì‚¬)
                query = text("""
                    SELECT id
                    FROM record_chunks
                    WHERE record_id = :record_id
                    ORDER BY embedding <=> cast(:embedding as vector)
                    LIMIT 3
                """)

                # embeddingì„ ë¬¸ìì—´ë¡œ ë³€í™˜ (PostgreSQL vector í˜•ì‹)
                embedding_str = str(query_embedding)

                result = db.execute(
                    query,
                    {"record_id": record_id, "embedding": embedding_str}
                )

                rows = result.fetchall()
                chunk_ids = [row[0] for row in rows]

                logger.info(f"Retrieved {len(chunk_ids)} chunk IDs for topic '{topic}' using vector similarity")
                return chunk_ids

            finally:
                if should_close:
                    db.close()

        except Exception as e:
            logger.error(f"Error searching chunks for topic {topic}: {e}")
            return []

    def _embed_text_sync(self, text: str) -> List[float]:
        """í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ì„ë² ë”© (ë™ê¸° ë²„ì „ - ì´ë²¤íŠ¸ ë£¨í”„ ë‚´ì—ì„œë„ ì•ˆì „í•˜ê²Œ ì‹¤í–‰)"""
        import threading
        import concurrent.futures

        def run_in_new_loop():
            """ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ìƒˆ ì´ë²¤íŠ¸ ë£¨í”„ ìƒì„±í•˜ì—¬ ì‹¤í–‰"""
            import asyncio
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                return loop.run_until_complete(self._embed_text(text))
            finally:
                loop.close()

        # ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ ì´ë²¤íŠ¸ ë£¨í”„ê°€ ìˆëŠ”ì§€ í™•ì¸
        try:
            import asyncio
            asyncio.get_running_loop()
            # ì‹¤í–‰ ì¤‘ì¸ ë£¨í”„ê°€ ìˆìœ¼ë©´ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
            with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                future = executor.submit(run_in_new_loop)
                return future.result(timeout=30)  # 30ì´ˆ íƒ€ì„ì•„ì›ƒ
        except RuntimeError:
            # ì‹¤í–‰ ì¤‘ì¸ ë£¨í”„ê°€ ì—†ìœ¼ë©´ ì§ì ‘ ì‹¤í–‰
            import asyncio
            return asyncio.run(self._embed_text(text))



vector_service = VectorService()
